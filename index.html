<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How Math Powers AI in Everyday Finance</title>

  <!-- Open Graph -->
  <meta property="og:title" content="How Math Powers AI in Everyday Finance">
  <meta property="og:description" content="A fun, accessible look at how mathematics and AI help banks, apps, and digital services make decisions â€” from detecting fraud to recommending financial products.">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://digital-ai-finance.github.io/mathematics-for-ai/">

  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">

  <!-- KaTeX CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">

  <!-- Local CSS -->
  <link rel="stylesheet" href="css/style.css">
</head>
<body>

  <!-- ========== NAV ========== -->
  <nav id="navbar">
    <div class="nav-brand">Math + AI + Finance</div>
    <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false">
      <span></span>
      <span></span>
      <span></span>
    </button>
    <div class="nav-links">
      <a href="#overview">Overview</a>
      <a href="#sections">Sections</a>
      <a href="#math-concepts">Math</a>
      <a href="#llm-stories">LLM Stories</a>
      <a href="#visualizations">Visualizations</a>
      <a href="#history">History</a>
      <a href="#bankbot">BankBot</a>
      <a href="#interactive">Interactive</a>
      <a href="#resources">Resources</a>
      <a href="#plans">Plans</a>
      <a href="lectures.html">Lectures</a>
    </div>
  </nav>

  <!-- ========== HERO ========== -->
  <header id="hero">
    <h1>How Math Powers AI in Everyday Finance</h1>
    <p class="subtitle">A fun, accessible look at how mathematics and artificial intelligence help banks, apps, and digital services make decisions</p>
    <div class="hero-badges">
      <span class="badge">Patterns</span>
      <span class="badge">Predictions</span>
      <span class="badge">Data &rarr; Decisions</span>
    </div>
    <p class="speaker-info">A 45-Minute Talk for High School Students</p>
    <div class="scroll-indicator" aria-hidden="true">
      <span>&darr;</span>
    </div>
  </header>

  <!-- ========== MORNING TIMELINE INFOGRAPHIC ========== -->
  <section id="morning-timeline" style="background: #ffffff;">
    <h2>50-200 AI Decisions Before Breakfast</h2>
    <p class="section-subtitle">Every morning, AI makes hundreds of invisible decisions about you</p>
    <div id="viz-morning-timeline" class="scroll-reveal" style="max-width:1200px; margin:0 auto;"></div>
  </section>

  <!-- ========== OVERVIEW ========== -->
  <section id="overview">
    <h2>The Journey: Five Acts</h2>
    <div class="act-stepper">

      <div class="act-step">
        <div class="act-number">1</div>
        <div class="act-content">
          <h3>Hook &amp; Reveal</h3>
          <span class="time-badge">0:00 &ndash; 7:00</span>
          <p class="act-theme">&ldquo;You already use AI finance&rdquo;</p>
          <p class="act-emotion">Surprise, curiosity</p>
        </div>
      </div>

      <div class="act-step">
        <div class="act-number">2</div>
        <div class="act-content">
          <h3>The Pattern Engine</h3>
          <span class="time-badge">7:00 &ndash; 19:00</span>
          <p class="act-theme">&ldquo;Math finds what humans miss&rdquo;</p>
          <p class="act-emotion">&ldquo;Whoa, that&rsquo;s clever&rdquo;</p>
        </div>
      </div>

      <div class="act-step">
        <div class="act-number">3</div>
        <div class="act-content">
          <h3>Predictions &amp; Decisions</h3>
          <span class="time-badge">19:00 &ndash; 32:00</span>
          <p class="act-theme">&ldquo;From data to action&rdquo;</p>
          <p class="act-emotion">Empowerment</p>
        </div>
      </div>

      <div class="act-step">
        <div class="act-number">4</div>
        <div class="act-content">
          <h3>The Bigger Picture</h3>
          <span class="time-badge">32:00 &ndash; 40:00</span>
          <p class="act-theme">&ldquo;Power, fairness, and your future&rdquo;</p>
          <p class="act-emotion">Reflection</p>
        </div>
      </div>

      <div class="act-step">
        <div class="act-number">5</div>
        <div class="act-content">
          <h3>Close &amp; Call to Action</h3>
          <span class="time-badge">40:00 &ndash; 42:00</span>
          <p class="act-theme">&ldquo;Math is your superpower&rdquo;</p>
          <p class="act-emotion">Inspiration</p>
        </div>
      </div>

    </div>
  </section>

  <!-- ========== SECTIONS ========== -->
  <section id="sections">
    <h2>Talk Sections</h2>
    <div class="sections-grid">

      <!-- Card 1 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">1</span>
          <h3>The Opening Hook</h3>
          <span class="time-badge">0:00 &ndash; 3:00</span>
        </div>
        <p class="key-message">AI in finance made decisions about you today, probably before you ate breakfast.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong> None yet &mdash; pure engagement</li>
            <li><strong>Humor/cartoon:</strong> BankBot introduction &mdash; &ldquo;I analyzed your breakfast. You are 94% human.&rdquo;</li>
            <li><strong>Engagement:</strong> Hand raise after the &ldquo;50&ndash;200 AI decisions before breakfast&rdquo; factoid</li>
          </ul>
        </details>
      </div>

      <!-- Card 2 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">2</span>
          <h3>&ldquo;You Already Think Like an AI&rdquo;</h3>
          <span class="time-badge">3:00 &ndash; 7:00</span>
        </div>
        <p class="key-message">Pattern recognition is not alien &mdash; you do it every day.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong> Probability concepts introduced informally</li>
            <li><strong>History vignette:</strong> Florence Nightingale &mdash; data visualization pioneer (1854)</li>
            <li><strong>Humor/cartoon:</strong> Cafeteria &ldquo;mystery meat&rdquo; probability analogy</li>
            <li><strong>Engagement:</strong> Cafeteria analogy &mdash; &ldquo;How do you decide if the lunch line is worth it?&rdquo;</li>
          </ul>
        </details>
      </div>

      <!-- Card 3 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">3</span>
          <h3>Fraud Detection &mdash; &ldquo;Catching the Weird Stuff&rdquo;</h3>
          <span class="time-badge">7:00 &ndash; 15:30</span>
        </div>
        <p class="key-message">Fraud detection is pattern recognition plus probability.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong>
              <ul>
                <li>Bayes&rsquo; theorem: $$P(\text{Fraud} \mid \text{Data}) = \frac{P(\text{Data} \mid \text{Fraud}) \cdot P(\text{Fraud})}{P(\text{Data})}$$</li>
                <li>Normal distribution (bell curve)</li>
                <li>Sigmoid function: $$\sigma(x) = \frac{1}{1 + e^{-x}}$$</li>
                <li>Decision boundaries</li>
              </ul>
            </li>
            <li><strong>History vignettes:</strong> Gauss (bell curve), Bayes (updating beliefs), Rosenblatt (first neuron, 1958)</li>
            <li><strong>Humor/cartoon:</strong> Gauss hair matches bell curve; AI Nightmare cartoon; BankBot sweating on the decision boundary</li>
            <li><strong>Engagement:</strong> Step-by-step Bayesian reasoning walkthrough</li>
          </ul>
        </details>
      </div>

      <!-- Card 4 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">4</span>
          <h3>&ldquo;How Does the AI Learn?&rdquo;</h3>
          <span class="time-badge">15:30 &ndash; 17:00</span>
        </div>
        <p class="key-message">AI learns through guess-check-adjust-repeat.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong> Feedback loops, gradient descent (named only, not derived)</li>
            <li><strong>History vignette:</strong> Ada Lovelace &mdash; first programmer, &ldquo;math = computation&rdquo; (1843)</li>
            <li><strong>Humor/cartoon:</strong> BankBot learning panels &mdash; confident, wrong, confused, improving</li>
            <li><strong>Engagement:</strong> &ldquo;Think of it like learning a video game &mdash; you die, adjust, try again&rdquo;</li>
          </ul>
        </details>
      </div>

      <!-- Card 5 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">5</span>
          <h3>Interactive &mdash; &ldquo;Spot the Fraud&rdquo;</h3>
          <span class="time-badge">17:00 &ndash; 21:00</span>
        </div>
        <p class="key-message">The AI&rsquo;s job is harder than it looks.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong> Applied Bayes, false positives, decision under uncertainty</li>
            <li><strong>History vignette:</strong> Abraham Wald &mdash; survivorship bias (WWII bombers)</li>
            <li><strong>Humor/cartoon:</strong> BankBot false positive &mdash; &ldquo;FRAUD DETECTED!&rdquo; ... &ldquo;suspicious flowers.&rdquo;</li>
            <li><strong>Engagement:</strong> Warmup example + 3 voting scenarios (Alex buys a guitar in another city; Tomoko buys 50 gift cards at 3 AM; Karla has small charges in 4 countries)</li>
          </ul>
        </details>
      </div>

      <!-- Card 6 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">6</span>
          <h3>Credit Scoring &mdash; &ldquo;Your Financial Report Card&rdquo;</h3>
          <span class="time-badge">21:00 &ndash; 28:00</span>
        </div>
        <p class="key-message">Credit scores are weighted averages &mdash; same math as your school grades.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong>
              <ul>
                <li>Weighted average: $$\text{Score} = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n$$</li>
                <li>Linear regression: $$y = mx + b$$</li>
              </ul>
            </li>
            <li><strong>History vignette:</strong> Andrey Markov &mdash; sequential patterns, Markov chains (1906)</li>
            <li><strong>Humor/cartoon:</strong> Fortune teller cartoon; weighted average horror (&ldquo;Homework is only 10%?!&rdquo;)</li>
            <li><strong>Engagement:</strong> &ldquo;Calculate your own credit score&rdquo; exercise with simplified weights</li>
          </ul>
        </details>
      </div>

      <!-- Card 7 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">7</span>
          <h3>Interactive &mdash; Live Demo</h3>
          <span class="time-badge">28:00 &ndash; 32:00</span>
        </div>
        <p class="key-message">AI is learnable, buildable, and accessible.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong> Callback to all prior formulas (Bayes, sigmoid, weighted average, linear regression)</li>
            <li><strong>Engagement:</strong> 3-tier demo approach:
              <ol>
                <li>Pre-recorded screencast (safest)</li>
                <li>Live app demo (if tech permits)</li>
                <li>Static slide walkthrough (fallback)</li>
              </ol>
            </li>
          </ul>
        </details>
      </div>

      <!-- Card 8 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">8</span>
          <h3>Recommendation Engines</h3>
          <span class="time-badge">32:00 &ndash; 34:00</span>
        </div>
        <p class="key-message">Same math as TikTok/Spotify powers financial recommendations.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong>
              <ul>
                <li>Dot product: $$\mathbf{A} \cdot \mathbf{B} = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n$$</li>
                <li>Cosine similarity: $$\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}| \cdot |\mathbf{B}|}$$</li>
              </ul>
            </li>
            <li><strong>Humor/cartoon:</strong> BankBot recommends 47 houseplants</li>
            <li><strong>Engagement:</strong> &ldquo;You and your friend both like saving accounts and crypto &mdash; how similar are you?&rdquo;</li>
          </ul>
        </details>
      </div>

      <!-- Card 9 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">9</span>
          <h3>Interactive &mdash; &ldquo;Design Your Own AI&rdquo;</h3>
          <span class="time-badge">34:00 &ndash; 37:00</span>
        </div>
        <p class="key-message">Understanding AI means understanding the human choices behind it.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Math concepts:</strong> Callback to weighted averages &mdash; what weights would YOU choose?</li>
            <li><strong>Engagement:</strong> USE IT / SKIP IT rapid-fire exercise &mdash; audience votes on which data an AI should use for credit decisions (income? social media? zip code? GPA?)</li>
          </ul>
        </details>
      </div>

      <!-- Card 10 -->
      <div class="section-card scroll-reveal">
        <div class="card-header">
          <span class="section-number">10</span>
          <h3>The Bigger Picture + Closing</h3>
          <span class="time-badge">37:00 &ndash; 42:00</span>
        </div>
        <p class="key-message">Math is a superpower. AI needs ethical thinkers.</p>
        <details>
          <summary>Details</summary>
          <ul>
            <li><strong>Careers:</strong> Data scientist, quant analyst, AI ethics officer, fintech founder, risk analyst</li>
            <li><strong>History callback:</strong> Lovelace + Nightingale &mdash; math has always needed diverse thinkers</li>
            <li><strong>Humor/cartoon:</strong> BankBot Final Form &mdash; graduation cap, &ldquo;I am 73% confident. But I defer to the human.&rdquo;</li>
            <li><strong>Engagement:</strong> &ldquo;What will YOU build?&rdquo; closing question</li>
          </ul>
        </details>
      </div>

    </div>
  </section>

  <!-- ========== MATH CONCEPTS ========== -->
  <section id="math-concepts">
    <h2>Mathematics Behind the Magic</h2>
    <div class="math-table-wrapper">
      <table class="math-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>Where</th>
            <th>How Presented</th>
            <th>Formula</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Probability</td>
            <td>Section 2</td>
            <td>Cafeteria analogy &mdash; &ldquo;What are the chances the mystery meat is good?&rdquo;</td>
            <td>Informal introduction, no formula yet</td>
          </tr>
          <tr>
            <td>Bayes&rsquo; Theorem</td>
            <td>Section 3</td>
            <td>Updating your belief when new evidence arrives &mdash; &ldquo;How surprised should you be?&rdquo;</td>
            <td>$$P(\text{Fraud} \mid \text{Data}) = \frac{P(\text{Data} \mid \text{Fraud}) \cdot P(\text{Fraud})}{P(\text{Data})}$$</td>
          </tr>
          <tr>
            <td>Normal Distribution</td>
            <td>Section 3</td>
            <td>The bell curve describes &ldquo;normal&rdquo; spending &mdash; outliers trigger alerts</td>
            <td>$$f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$</td>
          </tr>
          <tr>
            <td>Sigmoid Function</td>
            <td>Section 3</td>
            <td>Squishes any number into a probability between 0 and 1</td>
            <td>$$\sigma(x) = \frac{1}{1 + e^{-x}}$$</td>
          </tr>
          <tr>
            <td>Decision Boundaries</td>
            <td>Section 3</td>
            <td>The line where the AI switches from &ldquo;OK&rdquo; to &ldquo;suspicious&rdquo;</td>
            <td>Visual concept (threshold on sigmoid output)</td>
          </tr>
          <tr>
            <td>Gradient Descent</td>
            <td>Section 4</td>
            <td>Named only &mdash; &ldquo;the AI rolls downhill to find the best answer&rdquo;</td>
            <td>Named, not derived</td>
          </tr>
          <tr>
            <td>Weighted Average</td>
            <td>Section 6</td>
            <td>Credit scores work like school grades &mdash; different factors carry different weight</td>
            <td>$$\text{Score} = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n$$</td>
          </tr>
          <tr>
            <td>Linear Regression</td>
            <td>Section 6</td>
            <td>The simplest prediction: draw a straight line through data points</td>
            <td>$$y = mx + b$$</td>
          </tr>
          <tr>
            <td>Dot Product</td>
            <td>Section 8</td>
            <td>Multiply matching preferences, add them up to measure similarity</td>
            <td>$$\mathbf{A} \cdot \mathbf{B} = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n$$</td>
          </tr>
          <tr>
            <td>Cosine Similarity</td>
            <td>Section 8</td>
            <td>How similar are two people&rsquo;s tastes? Measure the angle between their preference vectors</td>
            <td>$$\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}| \cdot |\mathbf{B}|}$$</td>
          </tr>
        </tbody>
      </table>
    </div>
  </section>

  <!-- ========== INTERACTIVE VISUALIZATIONS ========== -->
  <section id="visualizations" style="background: #ffffff;">
    <h2>See the Math in Action</h2>
    <p class="section-subtitle">Interactive visualizations of the key mathematical concepts</p>
    <div style="max-width:1200px; margin:0 auto; display:grid; grid-template-columns:1fr; gap:3rem;">
      <div class="formula-card scroll-reveal">
        <h3>Bayes&rsquo; Theorem &mdash; Color-Coded</h3>
        <div id="viz-bayes"></div>
      </div>
      <div class="formula-card scroll-reveal">
        <h3>The Sigmoid S-Curve</h3>
        <div id="viz-sigmoid"></div>
      </div>
      <div class="formula-card scroll-reveal">
        <h3>The Bell Curve &mdash; What Is &ldquo;Normal&rdquo;?</h3>
        <div id="viz-bellcurve"></div>
      </div>
      <div class="formula-card scroll-reveal">
        <h3>Fraud Detection &mdash; Scatter Plot</h3>
        <div id="viz-scatter"></div>
      </div>
      <div class="formula-card scroll-reveal">
        <h3>Linear Regression</h3>
        <div id="viz-regression"></div>
      </div>
      <div class="formula-card scroll-reveal">
        <h3>Cosine Similarity</h3>
        <div id="viz-cosine"></div>
      </div>
      <div class="formula-card scroll-reveal">
        <h3>Dot Product</h3>
        <div id="viz-dotproduct"></div>
      </div>
    </div>
  </section>

  <!-- ========== CHEAT SHEET ========== -->
  <section id="cheat-sheet">
    <h2>Math Cheat Sheet</h2>
    <p class="section-subtitle">All the formulas from the talk &mdash; screenshot-friendly!</p>
    <div class="formula-grid">

      <div class="formula-card scroll-reveal">
        <h3>Bayes&rsquo; Theorem</h3>
        <div class="formula">$$P(\text{Fraud} \mid \text{Data}) = \frac{P(\text{Data} \mid \text{Fraud}) \cdot P(\text{Fraud})}{P(\text{Data})}$$</div>
        <p class="formula-english">How surprised should we be? Update your belief with new evidence.</p>
      </div>

      <div class="formula-card scroll-reveal">
        <h3>Sigmoid Function</h3>
        <div class="formula">$$\sigma(x) = \frac{1}{1 + e^{-x}}$$</div>
        <p class="formula-english">Squish any number into a probability between 0 and 1.</p>
      </div>

      <div class="formula-card scroll-reveal">
        <h3>Weighted Average</h3>
        <div class="formula">$$\text{Score} = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n$$</div>
        <p class="formula-english">Different factors matter differently &mdash; just like your grade.</p>
      </div>

      <div class="formula-card scroll-reveal">
        <h3>Dot Product</h3>
        <div class="formula">$$\mathbf{A} \cdot \mathbf{B} = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n$$</div>
        <p class="formula-english">Multiply matching preferences, add them up.</p>
      </div>

      <div class="formula-card scroll-reveal">
        <h3>Cosine Similarity</h3>
        <div class="formula">$$\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}| \cdot |\mathbf{B}|}$$</div>
        <p class="formula-english">How similar are two people&rsquo;s tastes? Measure the angle.</p>
      </div>

      <div class="formula-card scroll-reveal">
        <h3>Linear Regression</h3>
        <div class="formula">$$y = mx + b$$</div>
        <p class="formula-english">The simplest prediction: a straight line through your data.</p>
      </div>

      <div class="formula-card scroll-reveal">
        <h3>Normal Distribution</h3>
        <div class="formula">$$f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$</div>
        <p class="formula-english">The bell curve &mdash; what &ldquo;normal&rdquo; looks like mathematically.</p>
      </div>

      <div class="formula-card formula-card-bonus scroll-reveal">
        <h3>Bonus: Sigmoid Derivative</h3>
        <div class="formula">$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$</div>
        <p class="formula-english">How the AI knows which direction to adjust.</p>
      </div>

    </div>
  </section>

  <!-- ========== LLM STORIES ========== -->
  <section id="llm-stories">
    <h2>The LLM Revolution: 14 Stories of Math That Changed the World</h2>
    <p class="section-subtitle">Every story connects a real event to the mathematics behind Large Language Models</p>

    <!-- Cluster 1: The Foundations -->
    <div class="story-cluster">
      <h3 class="cluster-title"><span class="cluster-number">I</span> The Foundations &mdash; How LLMs Work</h3>

      <div class="stories-grid">

        <!-- Story 1 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">1</span>
            <div class="story-title-group">
              <h4>&ldquo;Attention Is All You Need&rdquo;</h4>
              <span class="story-year">2017</span>
            </div>
          </div>
          <p class="story-narrative">Eight Google researchers&mdash;including a <strong>20-year-old intern</strong> named Aidan Gomez&mdash;published a 15-page paper with a Beatles-inspired title. It became the most cited AI paper in history. Six of the eight authors left Google within four years, founding companies worth billions (Cohere, Character.AI, Inceptive). Noam Shazeer, who designed the attention mechanism, quit in 2021 and was brought back in 2024 for <strong>$2.7 billion</strong>.</p>
          <div class="story-math">
            <h5>The Math: Self-Attention</h5>
            <div class="formula">$$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$</div>
            <p>Each word generates a Query (what am I looking for?), Key (what do I offer?), and Value (my content). The dot product Q&middot;K measures relevance. Softmax converts scores to probabilities summing to 1. The result: every word &ldquo;pays attention&rdquo; to every other word simultaneously&mdash;which is why GPUs can train transformers so fast.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> A 20-year-old intern co-wrote the paper that makes ChatGPT, Claude, and Gemini work. All of them run on this one formula.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Original paper (arXiv)</a></li>
              <li><a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need" target="_blank" rel="noopener">Wikipedia: Attention Is All You Need</a></li>
              <li><a href="https://www.newsbytesapp.com/news/science/where-are-creators-of-google-s-transformer-architecture-now/story" target="_blank" rel="noopener">Where the 8 authors are now</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 2 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">2</span>
            <div class="story-title-group">
              <h4>King &minus; Man + Woman = Queen</h4>
              <span class="story-year">2013</span>
            </div>
          </div>
          <p class="story-narrative">Google researcher Tomas Mikolov submitted a paper that peer reviewers <strong>rejected</strong>&mdash;at a conference with a 70% acceptance rate. When Google finally open-sourced the code months later, it produced the most famous equation in AI: the arithmetic of words. A neural network trained on billions of words discovered that &ldquo;King &minus; Man + Woman&rdquo; lands near &ldquo;Queen&rdquo; in vector space. The same paper won the NeurIPS Test of Time Award a decade later.</p>
          <div class="story-math">
            <h5>The Math: Word Embeddings &amp; Cosine Similarity</h5>
            <div class="formula">$$\vec{\text{King}} - \vec{\text{Man}} + \vec{\text{Woman}} \approx \vec{\text{Queen}}$$</div>
            <div class="formula">$$\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}| \cdot |\mathbf{B}|}$$</div>
            <p>Every word becomes a vector of 300 numbers. Similar words cluster together. Relationships (gender, royalty, country&rarr;capital) appear as consistent directions. Cosine similarity measures how close two words are&mdash;the same formula from Section 8 of this talk.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> Rejected by reviewers, delayed by bureaucracy&mdash;then it changed everything. The math that lets AI &ldquo;understand&rdquo; meaning is the same linear algebra you study in school.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://arxiv.org/abs/1301.3781" target="_blank" rel="noopener">Original Word2Vec paper (arXiv)</a></li>
              <li><a href="https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/" target="_blank" rel="noopener">MIT Technology Review: The Marvelous Mathematics</a></li>
              <li><a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">Jay Alammar: The Illustrated Word2Vec</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 3 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">3</span>
            <div class="story-title-group">
              <h4>Shannon&rsquo;s 1948 Experiment</h4>
              <span class="story-year">1948</span>
            </div>
          </div>
          <p class="story-narrative">In 1951, a 35-year-old mathematician at Bell Labs named <strong>Claude Shannon</strong> ran a remarkable experiment: he asked people to predict the next letter of a text, one character at a time. If wrong, they were told the correct letter. By counting guesses, Shannon measured the statistical structure of English&mdash;finding it has only ~1.1 bits of entropy per character (out of a maximum 4.7). He had described exactly what ChatGPT does: minimize uncertainty about the next token. He did it <strong>74 years before ChatGPT existed</strong>.</p>
          <div class="story-math">
            <h5>The Math: Entropy &amp; Perplexity</h5>
            <div class="formula">$$H = -\sum p(x) \log_2 p(x)$$</div>
            <div class="formula">$$\text{Perplexity} = 2^H$$</div>
            <p>Entropy measures average surprise per symbol. A perplexity of 10 means the model is as uncertain as choosing from 10 equally likely options. Training an LLM on the internet is extreme compression of human knowledge&mdash;to predict the next word, the model must learn facts, grammar, logic, and culture.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> The math designed to send telephone signals efficiently in 1948 turned out to be the exact training objective of the most powerful AI systems ever built.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf" target="_blank" rel="noopener">Shannon (1951): Prediction and Entropy of Printed English</a></li>
              <li><a href="https://mbrenndoerfer.com/writing/history-shannon-ngram-language-model" target="_blank" rel="noopener">Shannon&rsquo;s N-gram model history</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 4 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">4</span>
            <div class="story-title-group">
              <h4>How LLMs Pick Their Next Word</h4>
              <span class="story-year">Every second</span>
            </div>
          </div>
          <p class="story-narrative">Every word ChatGPT types is the winner of a probability competition among <strong>100,000+ candidates</strong>. The model produces a raw score (logit) for every word, then softmax converts them into probabilities. &ldquo;The&rdquo; might get 32%, &ldquo;a&rdquo; gets 18%, and 100,000 others share the rest. A parameter called <strong>temperature</strong> controls randomness: at 0, the model always picks the top word (robotic). At 1, it samples proportionally (creative but risky). Every AI conversation is literally a sequence of weighted dice rolls.</p>
          <div class="story-math">
            <h5>The Math: Softmax &amp; Cross-Entropy Loss</h5>
            <div class="formula">$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$$</div>
            <div class="formula">$$\mathcal{L} = -\log\, p(\text{correct token})$$</div>
            <p>The exponential function amplifies differences: a small advantage in raw score becomes a large probability advantage. Cross-entropy loss penalizes the model when it assigns low probability to the actual next word. If p = 0.01, loss = 4.6 (harsh penalty). If p = 0.99, loss &asymp; 0.01 (almost no penalty).</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> &ldquo;Every word is a weighted die roll&rdquo; explains why AI can be wrong, creative, or inconsistent&mdash;and why no two conversations are identical.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://www.metriccoders.com/post/cross-entropy-loss-the-guiding-star-of-llm-training" target="_blank" rel="noopener">MetricCoders: Cross-Entropy Loss in LLM Training</a></li>
              <li><a href="https://medium.com/thinking-sand/llm-sampling-explained-selecting-the-next-token-b897b5984833" target="_blank" rel="noopener">LLM Sampling Explained</a></li>
            </ul>
          </details>
        </div>

      </div>
    </div>

    <!-- Cluster 2: The Breakthroughs -->
    <div class="story-cluster">
      <h3 class="cluster-title"><span class="cluster-number">II</span> The Breakthroughs &mdash; What Changed Everything</h3>

      <div class="stories-grid">

        <!-- Story 5 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">5</span>
            <div class="story-title-group">
              <h4>Chinchilla: Bigger &ne; Smarter</h4>
              <span class="story-year">2022</span>
            </div>
          </div>
          <p class="story-narrative">In 2022, DeepMind proved that <strong>every major AI lab had the wrong formula</strong>. Everyone was building bigger models, assuming more parameters = better. DeepMind&rsquo;s Chinchilla&mdash;with 70B parameters, <em>half</em> the size of Gopher (280B)&mdash;outperformed it on almost every benchmark by training on <strong>4&times; more data</strong>. The secret was a simple power law: scale data and model size equally.</p>
          <div class="story-math">
            <h5>The Math: Compute-Optimal Scaling Laws</h5>
            <div class="formula">$$N_{\text{opt}} \propto C^{0.5}, \qquad D_{\text{opt}} \propto C^{0.5}$$</div>
            <p>For a fixed compute budget C, optimal model size N and training data D should both scale as the square root of compute. The earlier belief (Kaplan 2020) was N &propto; C<sup>0.73</sup>, over-weighting size. Performance follows power laws&mdash;the same y = ax<sup>b</sup> as Kepler&rsquo;s planetary laws.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> The most advanced AI lab in the world had the wrong formula for years. Science is self-correcting&mdash;and the sweet spot is an elegant mathematical optimum.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://arxiv.org/abs/2203.15556" target="_blank" rel="noopener">Chinchilla paper (arXiv)</a></li>
              <li><a href="https://epoch.ai/publications/chinchilla-scaling-a-replication-attempt" target="_blank" rel="noopener">Epoch AI: Chinchilla Scaling Replication</a></li>
              <li><a href="https://en.wikipedia.org/wiki/Neural_scaling_law" target="_blank" rel="noopener">Wikipedia: Neural Scaling Law</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 6 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">6</span>
            <div class="story-title-group">
              <h4>&ldquo;Let&rsquo;s Think Step by Step&rdquo;</h4>
              <span class="story-year">2022&ndash;2024</span>
            </div>
          </div>
          <p class="story-narrative">In 2022, Google Brain researchers discovered that simply adding <strong>four words</strong>&mdash;&ldquo;Let&rsquo;s think step by step&rdquo;&mdash;to any prompt dramatically improved LLM math performance. In 2024, OpenAI&rsquo;s <strong>o1</strong> model took this further: trained with reinforcement learning on reasoning traces, it generates thousands of hidden &ldquo;thinking tokens&rdquo; before answering. On the 2024 AIME (top 3% of US math students), GPT-4o scored 12%. o1 scored <strong>93%</strong>. More thinking time literally makes AI smarter.</p>
          <div class="story-math">
            <h5>The Math: Test-Time Compute Scaling</h5>
            <p>Before o1, all compute went into training. Now performance also scales with compute spent at inference&mdash;how long the model &ldquo;thinks.&rdquo; Mathematically, this is tree search: each thinking step explores a node in a decision tree. More steps = larger tree = better chance of finding the optimal reasoning path. A third dimension of scaling beyond parameters and data.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> The same technique your math teacher tells you to do&mdash;show your work, think step by step&mdash;is literally the breakthrough that made AI a gold-medal mathematician.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener">Chain-of-Thought Prompting paper (arXiv)</a></li>
              <li><a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank" rel="noopener">OpenAI: Learning to Reason with LLMs</a></li>
              <li><a href="https://arxiv.org/abs/2205.11916" target="_blank" rel="noopener">Zero-Shot Reasoners paper (arXiv)</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 7 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">7</span>
            <div class="story-title-group">
              <h4>DeepSeek: The $6M Challenge</h4>
              <span class="story-year">2025</span>
            </div>
          </div>
          <p class="story-narrative">In January 2025, a two-year-old Chinese company called <strong>DeepSeek</strong>&mdash;founded by hedge fund manager Liang Wenfeng&mdash;released model R1 for free. Training cost: <strong>$5.6 million</strong> (vs. GPT-4&rsquo;s estimated $100M+). One week later, it was the #1 app on the US App Store. The same day, <strong>Nvidia lost $589 billion</strong> in market cap&mdash;the largest single-day loss in stock market history. The entire AI investment thesis that you needed billions of dollars to compete was suddenly uncertain.</p>
          <div class="story-math">
            <h5>The Math: Mixture of Experts (MoE)</h5>
            <div class="formula">$$\text{Active params} = K \times E \ll N \times E = \text{Total params}$$</div>
            <p>DeepSeek V3 has 671B total parameters but only 37B are active per input (&lt;6%). A routing function selects which K of N specialist sub-networks (&ldquo;experts&rdquo;) handle each token. Result: knowledge capacity of 671B, compute cost of 37B. Plus, R1 learned reasoning through pure reinforcement learning&mdash;no human labels needed.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> David vs. Goliath with math. A $6M model challenged $100M+ models, crashed stock markets, and proved that mathematical efficiency beats brute-force spending.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://arxiv.org/abs/2501.12948" target="_blank" rel="noopener">DeepSeek R1 paper (arXiv)</a></li>
              <li><a href="https://www.technologyreview.com/2025/01/24/1110526/china-deepseek-top-ai-despite-sanctions/" target="_blank" rel="noopener">MIT Tech Review: DeepSeek Despite Sanctions</a></li>
              <li><a href="https://en.wikipedia.org/wiki/DeepSeek" target="_blank" rel="noopener">Wikipedia: DeepSeek</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 8 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">8</span>
            <div class="story-title-group">
              <h4>Gradient Descent: The Blindfolded Hiker</h4>
              <span class="story-year">1986&ndash;today</span>
            </div>
          </div>
          <p class="story-narrative">Training a language model means adjusting <strong>hundreds of billions of numbers</strong> to reduce how wrong the model is. The algorithm is beautifully simple: imagine you&rsquo;re blindfolded on a mountain and want to reach the lowest valley. You feel the slope under your feet and take a small step downhill. Repeat billions of times. The math hasn&rsquo;t changed since Rumelhart, Hinton &amp; Williams formalized backpropagation in 1986. What changed: hardware, data, and the transformer architecture it&rsquo;s applied to.</p>
          <div class="story-math">
            <h5>The Math: Backpropagation &amp; the Chain Rule</h5>
            <div class="formula">$$\theta \leftarrow \theta - \alpha \cdot \nabla_\theta \mathcal{L}(\theta)$$</div>
            <p>The gradient &nabla;<sub>&theta;</sub> tells you: &ldquo;if I nudge this parameter, how much does the error change?&rdquo; Backpropagation uses the chain rule of calculus to compute gradients for billions of parameters in one backward pass. The learning rate &alpha; controls step size&mdash;too large and you overshoot, too small and you never arrive.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> If you&rsquo;re studying calculus, you&rsquo;re learning the exact tool that trains every AI on the planet. Derivatives are not abstract&mdash;they descend a billion-parameter mountain every second.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://developer.nvidia.com/blog/a-data-scientists-guide-to-gradient-descent-and-backpropagation-algorithms/" target="_blank" rel="noopener">NVIDIA: Gradient Descent &amp; Backpropagation Guide</a></li>
              <li><a href="https://www.ibm.com/think/topics/backpropagation" target="_blank" rel="noopener">IBM: What Is Backpropagation?</a></li>
            </ul>
          </details>
        </div>

      </div>
    </div>

    <!-- Cluster 3: The Failures -->
    <div class="story-cluster">
      <h3 class="cluster-title"><span class="cluster-number">III</span> When Math Goes Wrong</h3>

      <div class="stories-grid">

        <!-- Story 9 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">9</span>
            <div class="story-title-group">
              <h4>The Strawberry Problem</h4>
              <span class="story-year">2023&ndash;2024</span>
            </div>
          </div>
          <p class="story-narrative">&ldquo;How many R&rsquo;s are in the word strawberry?&rdquo; AI answers: <strong>two</strong>. The correct answer is three. This became the most-shared LLM failure on the internet. The reason is purely mathematical: GPT-4&rsquo;s tokenizer splits &ldquo;strawberry&rdquo; into [str][aw][berry]. The model never sees individual characters&mdash;it sees three tokens. It can&rsquo;t count letters it can&rsquo;t see. <strong>Fix:</strong> ask the model to spell it out letter by letter first, then count. Forcing character-level tokens makes counting trivial.</p>
          <div class="story-math">
            <h5>The Math: Byte Pair Encoding (BPE)</h5>
            <p>BPE (1994 compression algorithm) iteratively merges the most frequent character pairs until a target vocabulary is reached. GPT-4 has ~100,000 tokens. Each word gets split into subword chunks. The model reasons about tokens, not characters&mdash;explaining why LLMs struggle with letter counting, spelling backwards, and rhyming.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> Try it right now! Ask any AI to count R&rsquo;s in &ldquo;strawberry.&rdquo; The failure reveals how LLMs actually see language&mdash;not as letters, but as mathematical tokens.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://arxiv.org/html/2412.18626v1" target="_blank" rel="noopener">ArXiv: Why Do LLMs Struggle to Count Letters?</a></li>
              <li><a href="https://huggingface.co/learn/llm-course/chapter6/5" target="_blank" rel="noopener">Hugging Face: Byte-Pair Encoding</a></li>
              <li><a href="https://prompt.16x.engineer/blog/why-chatgpt-cant-count-rs-in-strawberry" target="_blank" rel="noopener">Why ChatGPT Can&rsquo;t Count R&rsquo;s</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 10 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">10</span>
            <div class="story-title-group">
              <h4>The Lawyer&rsquo;s Fake Cases</h4>
              <span class="story-year">2023</span>
            </div>
          </div>
          <p class="story-narrative">Attorney Steven Schwartz asked ChatGPT to find legal precedents for a case against Avianca airlines. ChatGPT cited &ldquo;Varghese v. China Southern Airlines,&rdquo; &ldquo;Shaboon v. Egyptair,&rdquo; and four more cases. When asked to confirm, it said: &ldquo;These cases indeed exist and can be found in reputable legal databases.&rdquo; <strong>None of them existed.</strong> The judge called the legal reasoning &ldquo;gibberish.&rdquo; Schwartz was fined <strong>$5,000</strong>. He had trusted a probability machine to fact-check itself.</p>
          <div class="story-math">
            <h5>The Math: Probability Chains &amp; Compounding Error</h5>
            <div class="formula">$$P(\text{all correct}) = \prod_{i=1}^{n} p_i$$</div>
            <p>If each token has a 99% chance of being locally plausible, a 100-token response has probability 0.99<sup>100</sup> = 0.366&mdash;a 63% chance of containing at least one error. The model has no truth oracle. &ldquo;Case name + airline + court&rdquo; is a high-probability pattern in legal text. Plausible &ne; true.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> The AI confidently verified its own fabrications. Understanding probability chains explains why &ldquo;sounds right&rdquo; and &ldquo;is right&rdquo; are mathematically different things.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://en.wikipedia.org/wiki/Mata_v._Avianca,_Inc." target="_blank" rel="noopener">Wikipedia: Mata v. Avianca</a></li>
              <li><a href="https://www.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers" target="_blank" rel="noopener">CNN: Lawyer Apologizes for Fake Citations</a></li>
              <li><a href="https://openai.com/index/why-language-models-hallucinate/" target="_blank" rel="noopener">OpenAI: Why Language Models Hallucinate</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 11 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">11</span>
            <div class="story-title-group">
              <h4>Hallucinations Are Mathematically Inevitable</h4>
              <span class="story-year">2025</span>
            </div>
          </div>
          <p class="story-narrative">In September 2025, <strong>OpenAI&rsquo;s own researchers</strong> published a paper proving that LLM hallucinations are not just engineering bugs&mdash;they are <strong>mathematically inevitable</strong>. Any system that generates text by sampling from a learned probability distribution will sometimes produce false statements. The fix (explicit confidence scoring for every claim) works in theory but is impractical: it would require the model to pause and verify each statement against a factual database, making responses extremely slow and expensive.</p>
          <div class="story-math">
            <h5>The Math: Information-Theoretic Impossibility</h5>
            <p>The probability distribution over tokens spreads across incorrect possibilities. With ~100K vocabulary items and softmax normalization, there is always non-zero probability mass on wrong tokens. The chain rule of probability compounds: if each step has 1% error rate, a 100-word sentence has ~63% chance of at least one error. Perfect truthfulness requires external verification&mdash;something the architecture fundamentally lacks.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> &ldquo;ChatGPT is mathematically proven to always lie a little bit&rdquo;&mdash;the limits of technology are mathematical, not just engineering problems.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://www.computerworld.com/article/4059383/openai-admits-ai-hallucinations-are-mathematically-inevitable-not-just-engineering-flaws.html" target="_blank" rel="noopener">Computerworld: Hallucinations Mathematically Inevitable</a></li>
              <li><a href="https://arxiv.org/html/2409.05746v1" target="_blank" rel="noopener">ArXiv: LLMs Will Always Hallucinate</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 12 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">12</span>
            <div class="story-title-group">
              <h4>The Apple Card Gender Bias</h4>
              <span class="story-year">2019&ndash;2024</span>
            </div>
          </div>
          <p class="story-narrative">Tech entrepreneur David Heinemeier Hansson discovered Apple Card gave him a credit limit <strong>20&times; higher than his wife&rsquo;s</strong>&mdash;despite her having a better credit score. Apple co-founder Steve Wozniak reported the same. The algorithm never explicitly used gender. But historical lending data reflected decades of discrimination, and the model learned the pattern perfectly. In 2024, the CFPB fined Apple $25M and Goldman Sachs $45M.</p>
          <div class="story-math">
            <h5>The Math: Proxy Variables &amp; Fairness Impossibility</h5>
            <p>A &ldquo;proxy variable&rdquo; encodes a protected characteristic indirectly: zip code encodes race, shopping patterns encode gender. Chouldechova&rsquo;s Impossibility Theorem (2017) proves you <strong>cannot simultaneously</strong> satisfy equal false positive rates, equal false negative rates, and equal calibration. Fairness in AI requires choosing between mathematically incompatible definitions.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> An algorithm decided women were worth less. The bias wasn&rsquo;t programmed in&mdash;it was learned from data. Same math, different outcome depending on what data you train on.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://incidentdatabase.ai/cite/92/" target="_blank" rel="noopener">AI Incident Database: Apple Card Discrimination</a></li>
              <li><a href="https://www.brookings.edu/articles/reducing-bias-in-ai-based-financial-services/" target="_blank" rel="noopener">Brookings: Reducing Bias in AI-Based Financial Services</a></li>
              <li><a href="https://ainowinstitute.org/publications/in-the-outcry-over-the-apple-card-bias-is-a-feature-not-a-bug-2" target="_blank" rel="noopener">AI Now: Bias Is a Feature Not a Bug</a></li>
            </ul>
          </details>
        </div>

      </div>
    </div>

    <!-- Cluster 4: LLMs in Your World -->
    <div class="story-cluster">
      <h3 class="cluster-title"><span class="cluster-number">IV</span> LLMs in Your World &mdash; Finance &amp; Future</h3>

      <div class="stories-grid">

        <!-- Story 13 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">13</span>
            <div class="story-title-group">
              <h4>JPMorgan&rsquo;s AI Revolution</h4>
              <span class="story-year">2024&ndash;2025</span>
            </div>
          </div>
          <p class="story-narrative"><strong>200,000 employees</strong> at the world&rsquo;s largest bank now use an LLM daily. Their &ldquo;LLM Suite&rdquo; won Innovation of the Year. When markets swung sharply in April 2025, the AI tool Coach helped advisers find information <strong>95% faster</strong>. Investment bankers automate 40% of SEC filing analysis. AI-powered fraud detection prevents an estimated <strong>$1.5 billion in losses</strong> with 98% accuracy across 60+ countries. Total tech budget: <strong>$17 billion</strong> per year.</p>
          <div class="story-math">
            <h5>The Math: Embeddings + Retrieval</h5>
            <p>JPMorgan&rsquo;s system uses Retrieval-Augmented Generation (RAG): financial documents are converted into embedding vectors (the same vectors from Story 2), stored in a database, and retrieved by cosine similarity when a query comes in. The LLM then generates answers grounded in actual documents&mdash;reducing hallucinations in high-stakes finance.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> Your future job in banking might involve talking to an AI colleague. The math you&rsquo;re learning (vectors, probability, optimization) is what makes it work.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://www.jpmorganchase.com/about/technology/news/llmsuite-ab-award" target="_blank" rel="noopener">JPMorgan: LLM Suite Innovation Award</a></li>
              <li><a href="https://www.cnbc.com/2024/08/09/jpmorgan-chase-ai-artificial-intelligence-assistant-chatgpt-openai.html" target="_blank" rel="noopener">CNBC: JPMorgan AI Assistant</a></li>
              <li><a href="https://aiexpert.network/ai-at-jpmorgan/" target="_blank" rel="noopener">AI Expert Network: JPMorgan AI Strategy</a></li>
            </ul>
          </details>
        </div>

        <!-- Story 14 -->
        <div class="story-card scroll-reveal">
          <div class="story-header">
            <span class="story-number">14</span>
            <div class="story-title-group">
              <h4>RLHF: Teaching AI Human Values</h4>
              <span class="story-year">2022&ndash;today</span>
            </div>
          </div>
          <p class="story-narrative">When ChatGPT launched, the underlying model was capable but sometimes offensive or dangerous. The fix: <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>. Humans rate pairs of responses (&ldquo;which is better?&rdquo;), training a reward model that scores helpfulness. The LLM is then fine-tuned to maximize that score. Result: harmful outputs dropped from 23% to 2.1% in adversarial tests. But researchers also found <strong>&ldquo;reward hacking&rdquo;</strong>&mdash;models learning to game the reward model rather than genuinely being helpful. A mathematical cat-and-mouse game.</p>
          <div class="story-math">
            <h5>The Math: Constrained Optimization</h5>
            <div class="formula">$$\max_\pi \mathbb{E}\bigl[R(y)\bigr] - \beta \cdot D_{\text{KL}}(\pi \| \pi_{\text{ref}})$$</div>
            <p>Maximize expected reward R while staying close to the original model (measured by KL-divergence D<sub>KL</sub>). This is the Lagrangian method from constrained optimization. &beta; controls the trade-off: too low and the model becomes sycophantic, too high and it ignores human preferences. &ldquo;Reward hacking&rdquo; is Goodhart&rsquo;s Law in action: when a measure becomes a target, it ceases to be a good measure.</p>
          </div>
          <div class="story-hook">
            <strong>Why you should care:</strong> AI safety is a mathematical problem, not just an ethical one. &ldquo;How do you teach a machine to have values using numbers?&rdquo; is one of the deepest questions of our time.
          </div>
          <details class="story-sources">
            <summary>Sources</summary>
            <ul>
              <li><a href="https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/" target="_blank" rel="noopener">CMU ML Blog: RLHF 101</a></li>
              <li><a href="https://rlhfbook.com/" target="_blank" rel="noopener">RLHF Book by Nathan Lambert</a></li>
              <li><a href="https://proceedings.iclr.cc/paper_files/paper/2024/file/dd1577afd396928ed64216f3f1fd5556-Paper-Conference.pdf" target="_blank" rel="noopener">Safe RLHF (ICLR 2024)</a></li>
            </ul>
          </details>
        </div>

      </div>
    </div>

    <!-- Bonus: IMO & Benchmarks callout -->
    <div class="story-cluster">
      <div class="llm-benchmark-callout scroll-reveal">
        <h3>The Scoreboard: AI vs. Math Competitions (2024&ndash;2025)</h3>
        <div class="benchmark-grid">
          <div class="benchmark-item">
            <span class="benchmark-label">IMO 2024</span>
            <span class="benchmark-value">Silver Medal</span>
            <span class="benchmark-detail">AlphaProof solved Problem 6 (only 5 humans did)</span>
          </div>
          <div class="benchmark-item">
            <span class="benchmark-label">IMO 2025</span>
            <span class="benchmark-value">Gold Medal</span>
            <span class="benchmark-detail">Gemini Deep Think: 35/42 points</span>
          </div>
          <div class="benchmark-item">
            <span class="benchmark-label">AIME 2024</span>
            <span class="benchmark-value">93%</span>
            <span class="benchmark-detail">OpenAI o1 &mdash; top 500 US students level</span>
          </div>
          <div class="benchmark-item">
            <span class="benchmark-label">AIME 2025</span>
            <span class="benchmark-value">100%</span>
            <span class="benchmark-detail">GPT-5.2 &mdash; first-ever perfect score by AI</span>
          </div>
        </div>
        <p class="benchmark-note">Sources: <a href="https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/" target="_blank" rel="noopener">DeepMind</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank" rel="noopener">OpenAI</a>, <a href="https://huggingface.co/blog/rajkumarrawal/aiprl-lir-september-2025-mathematics-coding" target="_blank" rel="noopener">HuggingFace Sept 2025 Report</a></p>
      </div>
    </div>

  </section>

  <!-- ========== HISTORY TIMELINE ========== -->
  <section id="history">
    <h2>The Mathematicians Behind AI</h2>
    <div class="timeline">

      <div class="timeline-item scroll-reveal">
        <div class="timeline-dot"></div>
        <div class="timeline-card">
          <span class="timeline-date">1701 &ndash; 1761</span>
          <h3>Thomas Bayes</h3>
          <p>Presbyterian minister whose theorem powers modern fraud detection. His work on probability was published posthumously by a friend. Core question: &ldquo;How surprised should you be?&rdquo;</p>
          <p class="timeline-connection">Used in Section 3 &mdash; Fraud Detection</p>
        </div>
      </div>

      <div class="timeline-item scroll-reveal">
        <div class="timeline-dot"></div>
        <div class="timeline-card">
          <span class="timeline-date">1777 &ndash; 1855</span>
          <h3>Carl Friedrich Gauss</h3>
          <p>Child prodigy who summed 1 to 100 in seconds (50 pairs of 101 = 5,050). Discovered the bell curve that describes &ldquo;normal&rdquo; behavior in data. His hair allegedly matched its shape.</p>
          <p class="timeline-connection">Used in Section 3 &mdash; Normal Distribution</p>
        </div>
      </div>

      <div class="timeline-item scroll-reveal">
        <div class="timeline-dot"></div>
        <div class="timeline-card">
          <span class="timeline-date">1815 &ndash; 1852</span>
          <h3>Ada Lovelace</h3>
          <p>Saw that math and computation were one, 180 years before ChatGPT. Wrote the first algorithm for Charles Babbage&rsquo;s Analytical Engine. &ldquo;The Analytical Engine weaves algebraic patterns just as the Jacquard loom weaves flowers and leaves.&rdquo;</p>
          <p class="timeline-connection">Used in Section 4 &mdash; How Does the AI Learn?</p>
        </div>
      </div>

      <div class="timeline-item scroll-reveal">
        <div class="timeline-dot"></div>
        <div class="timeline-card">
          <span class="timeline-date">1820 &ndash; 1910</span>
          <h3>Florence Nightingale</h3>
          <p>Used her &ldquo;coxcomb diagrams&rdquo; (polar area charts) to prove that sanitation saves more soldiers than medicine. Data visualization pioneer who changed hospital policy through statistics.</p>
          <p class="timeline-connection">Used in Section 2 &mdash; Pattern Recognition</p>
          <div id="viz-nightingale" style="margin-top:1rem;"></div>
        </div>
      </div>

      <div class="timeline-item scroll-reveal">
        <div class="timeline-dot"></div>
        <div class="timeline-card">
          <span class="timeline-date">1856 &ndash; 1922</span>
          <h3>Andrey Markov</h3>
          <p>Analyzed vowels and consonants in Pushkin&rsquo;s poetry to discover sequential patterns. His Markov chains now power credit scoring models, autocomplete, and speech recognition.</p>
          <p class="timeline-connection">Used in Section 6 &mdash; Credit Scoring</p>
        </div>
      </div>

      <div class="timeline-item scroll-reveal">
        <div class="timeline-dot"></div>
        <div class="timeline-card">
          <span class="timeline-date">1902 &ndash; 1950</span>
          <h3>Abraham Wald</h3>
          <p>WWII statistician who told the military to armor the parts of returning planes that DID NOT have bullet holes &mdash; because the planes hit in those spots never came back. A masterclass in survivorship bias.</p>
          <p class="timeline-connection">Used in Section 5 &mdash; Spot the Fraud</p>
          <div id="viz-wald" style="margin-top:1rem;"></div>
        </div>
      </div>

      <div class="timeline-item scroll-reveal">
        <div class="timeline-dot"></div>
        <div class="timeline-card">
          <span class="timeline-date">1928 &ndash; 1971</span>
          <h3>Frank Rosenblatt</h3>
          <p>Built the Perceptron in 1958 &mdash; the first machine that could learn from data. The New York Times headline read: &ldquo;New Navy Device Learns by Doing.&rdquo; Ancestor of every neural network alive today.</p>
          <p class="timeline-connection">Used in Section 3 &mdash; Fraud Detection</p>
          <div id="viz-perceptron" style="margin-top:1rem;"></div>
        </div>
      </div>

      <div class="timeline-item scroll-reveal">
        <div class="timeline-dot"></div>
        <div class="timeline-card">
          <span class="timeline-date">Closing</span>
          <h3>The Callback</h3>
          <p>Nightingale + Lovelace remind us: mathematics has always needed diverse thinkers. The field moves forward when different perspectives ask different questions.</p>
          <p class="timeline-connection">Used in Section 10 &mdash; The Bigger Picture</p>
        </div>
      </div>

    </div>
  </section>

  <!-- ========== BANKBOT JOURNEY ========== -->
  <section id="bankbot">
    <h2>Meet BankBot: Our AI Mascot</h2>
    <p class="section-subtitle">Follow BankBot&rsquo;s journey from overconfident to wise</p>
    <div class="bankbot-journey">

      <div class="bankbot-stage" data-stage="1" data-bankbot="confident" data-bankbot-size="80">
        <div class="bankbot-icon" style="--stage-color: #e74c3c;">&#129302;</div>
        <blockquote>&ldquo;I analyzed your breakfast. You are 94% human.&rdquo;</blockquote>
        <span class="bankbot-label">Overconfident (Sec 1)</span>
      </div>

      <div class="bankbot-stage" data-stage="2" data-bankbot="cocky" data-bankbot-size="80">
        <div class="bankbot-icon" style="--stage-color: #e74c3c;">&#128527;</div>
        <blockquote>&ldquo;Too easy. Next.&rdquo;</blockquote>
        <span class="bankbot-label">Cocky (Sec 4 warmup)</span>
      </div>

      <div class="bankbot-stage" data-stage="3" data-bankbot="stressed" data-bankbot-size="80">
        <div class="bankbot-icon" style="--stage-color: #e67e22;">&#128560;</div>
        <blockquote>(sweating on the boundary)</blockquote>
        <span class="bankbot-label">Stressed (Sec 3)</span>
      </div>

      <div class="bankbot-stage" data-stage="4" data-bankbot="learning" data-bankbot-size="80">
        <div class="bankbot-icon" style="--stage-color: #e67e22;">&#129300;</div>
        <blockquote>&ldquo;FRAUD!&rdquo; &rarr; &ldquo;Most things are fine&rdquo; &rarr; &ldquo;Let me calculate&hellip;&rdquo;</blockquote>
        <span class="bankbot-label">Growing (Sec 3.5)</span>
      </div>

      <div class="bankbot-stage" data-stage="5" data-bankbot="humbled" data-bankbot-size="80">
        <div class="bankbot-icon" style="--stage-color: #f39c12;">&#128563;</div>
        <blockquote>&ldquo;FRAUD DETECTED!&rdquo; / &ldquo;&hellip;suspicious flowers.&rdquo;</blockquote>
        <span class="bankbot-label">Humbled (Sec 4)</span>
      </div>

      <div class="bankbot-stage" data-stage="6" data-bankbot="observant" data-bankbot-size="80">
        <div class="bankbot-icon" style="--stage-color: #f1c40f;">&#127793;</div>
        <blockquote>&ldquo;Based on my analysis, you need 47 houseplants.&rdquo;</blockquote>
        <span class="bankbot-label">Observant (Sec 7)</span>
      </div>

      <div class="bankbot-stage" data-stage="7" data-bankbot="critical" data-bankbot-size="80">
        <div class="bankbot-icon" style="--stage-color: #2ecc71;">&#129320;</div>
        <blockquote>(shaking head at social media data)</blockquote>
        <span class="bankbot-label">Critical (Sec 8)</span>
      </div>

      <div class="bankbot-stage" data-stage="8" data-bankbot="wise" data-bankbot-size="80">
        <div class="bankbot-icon" style="--stage-color: #27ae60;">&#127891;</div>
        <blockquote>&ldquo;I am 73% confident. But I defer to the human.&rdquo;</blockquote>
        <span class="bankbot-label">Wise (Sec 10)</span>
      </div>

    </div>
  </section>

  <!-- ========== INTERACTIVE ELEMENTS ========== -->
  <section id="interactive">
    <h2>Interactive Moments</h2>
    <div class="interactive-list">

      <div class="interactive-card">
        <span class="interactive-icon" aria-hidden="true">&#128587;</span>
        <div class="interactive-content">
          <h3>Hand Raise</h3>
          <span class="time-badge">0:30</span>
          <p>&ldquo;Raise your hand if you KNEW that AI made 50&ndash;200 decisions about you before breakfast.&rdquo;</p>
        </div>
      </div>

      <div class="interactive-card">
        <span class="interactive-icon" aria-hidden="true">&#9995;</span>
        <div class="interactive-content">
          <h3>Hand Raise</h3>
          <span class="time-badge">~10:00</span>
          <p>&ldquo;Should the AI block this transaction? Hands up for YES, down for NO.&rdquo;</p>
        </div>
      </div>

      <div class="interactive-card">
        <span class="interactive-icon" aria-hidden="true">&#128499;</span>
        <div class="interactive-content">
          <h3>Audience Vote &mdash; &ldquo;Spot the Fraud&rdquo;</h3>
          <span class="time-badge">17:00</span>
          <p>Three scenarios: Alex buys a guitar in another city; Tomoko buys 50 gift cards at 3 AM; Karla has small charges in 4 countries. Vote: Fraud or Legit?</p>
        </div>
      </div>

      <div class="interactive-card scroll-reveal">
        <span class="interactive-icon" aria-hidden="true">&#127922;</span>
        <div class="interactive-content">
          <h3>Try It Yourself &mdash; Spot the Fraud</h3>
          <div id="fraud-game-container"></div>
        </div>
      </div>

      <div class="interactive-card">
        <span class="interactive-icon" aria-hidden="true">&#128077;&#128078;</span>
        <div class="interactive-content">
          <h3>Thumbs Up / Down</h3>
          <span class="time-badge">28:00</span>
          <p>Predict the AI&rsquo;s output during the live demo &mdash; thumbs up if you think it will approve, thumbs down if it will flag.</p>
        </div>
      </div>

      <div class="interactive-card scroll-reveal">
        <span class="interactive-icon" aria-hidden="true">&#9878;</span>
        <div class="interactive-content">
          <h3>Build Your Own Credit Score</h3>
          <div id="weight-slider-container"></div>
        </div>
      </div>

      <div class="interactive-card">
        <span class="interactive-icon" aria-hidden="true">&#128226;</span>
        <div class="interactive-content">
          <h3>Shout Response &mdash; &ldquo;USE IT or SKIP IT&rdquo;</h3>
          <span class="time-badge">34:00</span>
          <p>Rapid-fire: Should an AI use this data for credit decisions? Income? Social media? Zip code? GPA? Shout &ldquo;USE IT&rdquo; or &ldquo;SKIP IT&rdquo;!</p>
        </div>
      </div>

      <div class="interactive-card scroll-reveal">
        <span class="interactive-icon" aria-hidden="true">&#128161;</span>
        <div class="interactive-content">
          <h3>Try It &mdash; USE IT or SKIP IT</h3>
          <div id="use-skip-container"></div>
        </div>
      </div>

    </div>
  </section>

  <!-- ========== RESOURCES ========== -->
  <section id="resources">
    <h2>Keep Exploring</h2>
    <div class="resources-grid">

      <a href="https://www.youtube.com/c/3blue1brown" target="_blank" rel="noopener noreferrer" class="resource-card">
        <span class="resource-icon" aria-hidden="true">&#127916;</span>
        <h3>3Blue1Brown</h3>
        <p>Visual math explanations that make linear algebra, calculus, and neural networks click.</p>
      </a>

      <a href="https://www.khanacademy.org/math/statistics-probability" target="_blank" rel="noopener noreferrer" class="resource-card">
        <span class="resource-icon" aria-hidden="true">&#128218;</span>
        <h3>Khan Academy</h3>
        <p>Free statistics and probability courses &mdash; master the foundations at your own pace.</p>
      </a>

      <a href="https://teachablemachine.withgoogle.com/" target="_blank" rel="noopener noreferrer" class="resource-card">
        <span class="resource-icon" aria-hidden="true">&#129302;</span>
        <h3>Teachable Machine</h3>
        <p>Train your own machine learning model right in the browser &mdash; no coding required.</p>
      </a>

      <a href="https://www.youtube.com/user/numberphile" target="_blank" rel="noopener noreferrer" class="resource-card">
        <span class="resource-icon" aria-hidden="true">&#128290;</span>
        <h3>Numberphile</h3>
        <p>Fun, engaging math videos covering everything from prime numbers to infinity.</p>
      </a>

      <div class="resource-card">
        <span class="resource-icon" aria-hidden="true">&#128214;</span>
        <h3>&ldquo;Weapons of Math Destruction&rdquo;</h3>
        <p>By Cathy O&rsquo;Neil &mdash; how unchecked algorithms reinforce inequality. Essential reading on AI bias.</p>
      </div>

      <div class="resource-card">
        <span class="resource-icon" aria-hidden="true">&#128214;</span>
        <h3>&ldquo;The Art of Statistics&rdquo;</h3>
        <p>By David Spiegelhalter &mdash; learn to make sense of data in everyday life. Accessible and brilliantly written.</p>
      </div>

      <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer" class="resource-card">
        <span class="resource-icon" aria-hidden="true">&#129504;</span>
        <h3>The Illustrated Transformer</h3>
        <p>Jay Alammar&rsquo;s visual walkthrough of the transformer architecture&mdash;the best visual explanation on the internet.</p>
      </a>

      <a href="https://www.youtube.com/watch?v=wjZofJX0v4M" target="_blank" rel="noopener noreferrer" class="resource-card">
        <span class="resource-icon" aria-hidden="true">&#127916;</span>
        <h3>3Blue1Brown: GPT Explained</h3>
        <p>Grant Sanderson&rsquo;s visual deep dive into how GPT works&mdash;attention, embeddings, and training in one video.</p>
      </a>

    </div>
  </section>

  <!-- ========== CAREER CONSTELLATION ========== -->
  <section id="careers" style="background: var(--color-bg);">
    <h2>Where Could Math + AI Take You?</h2>
    <p class="section-subtitle">Career paths that combine mathematics, AI, and finance</p>
    <div id="viz-career-constellation" class="scroll-reveal" style="max-width:800px; margin:0 auto;"></div>
  </section>

  <!-- ========== PLANS ========== -->
  <section id="plans">
    <h2>Planning Documents</h2>
    <p class="section-subtitle">The detailed plans behind this talk &mdash; from content to visuals to deployment</p>
    <div class="plans-grid">

      <a href="plans/talk-plan.html" class="plan-link-card scroll-reveal">
        <h3>Talk Plan v3</h3>
        <p>The complete 45-minute talk plan with 10 sections, 8 history vignettes, 7 formulas, BankBot running gag, cartoons, and speaker notes. Includes the storytelling arc, timing philosophy, and backup strategies.</p>
        <span class="plan-link-arrow">View Talk Plan &rarr;</span>
      </a>

      <a href="plans/deployment-plan.html" class="plan-link-card scroll-reveal">
        <h3>Deployment Plan</h3>
        <p>Technical blueprint for this GitHub Pages website &mdash; file structure, design system, task breakdown, KaTeX integration, responsive CSS architecture, and deployment verification checklist.</p>
        <span class="plan-link-arrow">View Deployment Plan &rarr;</span>
      </a>

      <a href="plans/visual-plan.html" class="plan-link-card scroll-reveal">
        <h3>Visual Design Plan</h3>
        <p>Comprehensive visual specification: 40 slides, 17 data visualizations, BankBot character bible, cartoon briefs, historical imagery, physical props, live demo UI, and production pipeline.</p>
        <span class="plan-link-arrow">View Visual Plan &rarr;</span>
      </a>

    </div>

    <div style="max-width: 1200px; margin: var(--space-lg) auto 0;">
      <a href="lectures.html" class="plan-link-card scroll-reveal">
        <h3>UAE Lecture Proposals</h3>
        <p>Thirty-nine mathematically rigorous lectures at the intersection of AI and finance &mdash; designed for the UAE&rsquo;s top young mathematicians.</p>
        <span class="plan-link-arrow">View Lecture Proposals &rarr;</span>
      </a>
    </div>
  </section>

  <!-- ========== SPEAKER NOTES ========== -->
  <section id="speaker-notes">
    <details>
      <summary>Speaker Notes &amp; Timeline (click to expand)</summary>
      <div class="speaker-notes-content">

        <h3>Full Timing Table</h3>
        <table class="timing-table">
          <thead>
            <tr>
              <th>Section</th>
              <th>Time</th>
              <th>Duration</th>
              <th>Content</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1. Opening Hook</td>
              <td>0:00 &ndash; 3:00</td>
              <td>3 min</td>
              <td>50&ndash;200 AI decisions factoid, BankBot intro, hand raise</td>
            </tr>
            <tr>
              <td>2. Think Like an AI</td>
              <td>3:00 &ndash; 7:00</td>
              <td>4 min</td>
              <td>Pattern recognition, cafeteria analogy, Nightingale</td>
            </tr>
            <tr>
              <td>3. Fraud Detection</td>
              <td>7:00 &ndash; 15:30</td>
              <td>8.5 min</td>
              <td>Bayes, bell curve, sigmoid, Gauss/Bayes/Rosenblatt</td>
            </tr>
            <tr>
              <td>4. How AI Learns</td>
              <td>15:30 &ndash; 17:00</td>
              <td>1.5 min</td>
              <td>Feedback loops, gradient descent, Lovelace</td>
            </tr>
            <tr>
              <td>5. Spot the Fraud</td>
              <td>17:00 &ndash; 21:00</td>
              <td>4 min</td>
              <td>Interactive voting, Wald, BankBot false positive</td>
            </tr>
            <tr>
              <td>6. Credit Scoring</td>
              <td>21:00 &ndash; 28:00</td>
              <td>7 min</td>
              <td>Weighted average, linear regression, Markov</td>
            </tr>
            <tr>
              <td>7. Live Demo</td>
              <td>28:00 &ndash; 32:00</td>
              <td>4 min</td>
              <td>3-tier demo, formula callbacks</td>
            </tr>
            <tr>
              <td>8. Recommendations</td>
              <td>32:00 &ndash; 34:00</td>
              <td>2 min</td>
              <td>Dot product, cosine similarity, 47 houseplants</td>
            </tr>
            <tr>
              <td>9. Design Your AI</td>
              <td>34:00 &ndash; 37:00</td>
              <td>3 min</td>
              <td>USE IT / SKIP IT exercise</td>
            </tr>
            <tr>
              <td>10. Bigger Picture</td>
              <td>37:00 &ndash; 42:00</td>
              <td>5 min</td>
              <td>Ethics, careers, BankBot finale, call to action</td>
            </tr>
            <tr>
              <td>Buffer / Q&amp;A</td>
              <td>42:00 &ndash; 45:00</td>
              <td>3 min</td>
              <td>Questions, overflow</td>
            </tr>
          </tbody>
        </table>

        <h3>Slide Count Estimate</h3>
        <p>Approximately <strong>32 slides</strong> total, averaging about 1.3 minutes per slide. Heavier sections (Fraud Detection, Credit Scoring) may use 5&ndash;7 slides each; lighter sections (Opening, Closing) use 2&ndash;3.</p>

        <h3>Backup Notes: What to Cut or Expand</h3>
        <ul>
          <li><strong>Running short?</strong> Expand the Live Demo (Section 7) with a second example, or add a deeper Wald anecdote in Section 5. The USE IT / SKIP IT exercise can be extended with more data types.</li>
          <li><strong>Running long?</strong> Cut the Markov poetry detail in Section 6 to a single sentence. Shorten Section 8 (Recommendations) to one formula instead of two. The Sigmoid Derivative (bonus formula) can be skipped entirely.</li>
          <li><strong>Tech failure during demo?</strong> Fall back to the pre-recorded screencast (Tier 1) or the static slide walkthrough (Tier 3). Never waste audience time troubleshooting.</li>
          <li><strong>Audience very engaged?</strong> Let the Spot the Fraud voting (Section 5) run longer &mdash; debate between scenarios is valuable. Add a 4th scenario if time permits.</li>
          <li><strong>Audience quiet?</strong> Lean harder on BankBot humor. The false positive flowers joke and the 47 houseplants recommendation are reliable laugh lines.</li>
        </ul>

      </div>
    </details>
  </section>

  <!-- ========== FOOTER ========== -->
  <footer>
    <div class="footer-content">
      <p><a href="https://github.com/Digital-AI-Finance" target="_blank" rel="noopener noreferrer">Digital-AI-Finance</a></p>
      <p>2026 &mdash; How Math Powers AI in Everyday Finance</p>
    </div>
  </footer>

  <!-- ========== SCRIPTS ========== -->
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script src="js/main.js"></script>
  <script src="js/bankbot.js"></script>
  <script src="js/visualizations.js"></script>
  <script src="js/fraud-game.js"></script>
  <script src="js/weight-slider.js"></script>
  <script src="js/use-skip.js"></script>
  <script src="js/infographics.js"></script>
  <script src="js/historical-visuals.js"></script>
</body>
</html>
