<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lecture 8: The Code of the Universe &mdash; From Classical Mathematics to Large Language Models</title>

  <!-- Reveal.js 5.x -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reveal.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/theme/black.css" id="theme">

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">

  <!-- Custom 3B1B theme (overrides black.css) -->
  <link rel="stylesheet" href="css/theme-3b1b.css">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>

<div class="reveal">
<div class="slides">

<!-- ============================================================
     SLIDE 1 — Title
     ============================================================ -->
<section class="center-layout">
  <p><span class="milestone-badge milestone-ai-connection">Lecture 8</span></p>
  <h1>The Code of the Universe</h1>
  <h3 class="hl-teal" style="font-weight:500;">From Classical Mathematics to Large Language Models</h3>
  <p class="hl-muted text-sm mt-lg">How 300 Years of Pure Mathematics Built the AI You Use Every Day</p>

  <aside class="notes">
    Today we trace how three centuries of pure mathematics converged into the
    architecture of large language models. The central thread is graph theory
    &mdash; from Euler&rsquo;s 1736 proof on K&ouml;nigsberg to the attention
    mechanism in modern transformers. Linear algebra, probability, and information
    theory each enter the story at precise points, and by the final slide you
    will see all three branches meet in a single equation. This is another line
    of the code of the universe &mdash; written in the language of connections.
  </aside>
</section>


<!-- ============================================================
     SLIDE 2 — The LLM Revolution Runs on Graphs
     ============================================================ -->
<section class="center-layout">
  <h2>The Biggest Breakthrough in AI History</h2>

  <p class="fragment fade-in text-lg" data-fragment-index="1" style="margin-top:0.3rem;">
    ChatGPT. Claude. Gemini. &mdash; Built from <strong>three branches</strong> of pure mathematics.
  </p>

  <div class="columns-3" style="margin-top:0.5rem;">
    <div class="card card-teal fragment fade-in" data-fragment-index="2">
      <h4 style="color:var(--teal);">Graph Theory</h4>
      <p class="text-sm mb-0">Euler, 1736 &mdash; <em>The mathematics of connections.</em><br>Every LLM is a graph processor: attention creates a complete graph over tokens.</p>
    </div>
    <div class="card card-yellow fragment fade-in" data-fragment-index="3">
      <h4 style="color:var(--yellow);">Linear Algebra</h4>
      <p class="text-sm mb-0">Cayley, Sylvester &mdash; <em>The mathematics of transformation.</em><br>Embeddings, projections, the QKV matrices &mdash; linear algebra is the engine.</p>
    </div>
    <div class="card card-green fragment fade-in" data-fragment-index="4">
      <h4 style="color:var(--green);">Information Theory</h4>
      <p class="text-sm mb-0">Shannon, 1948 &mdash; <em>The mathematics of meaning.</em><br>Cross-entropy loss, tokenization, compression &mdash; how models learn from data.</p>
    </div>
  </div>

  <p class="fragment fade-up hl-yellow text-lg mt-lg" data-fragment-index="5">
    Today we trace one deep thread &mdash; <strong>graph theory</strong> &mdash; and watch all three converge in a single equation.
  </p>

  <aside class="notes">
    Three branches of mathematics converge here. Graph theory supplies the
    architecture: self-attention constructs a complete weighted digraph over
    tokens at every layer. Linear algebra supplies the engine: the QKV
    projections are learned linear maps in $\mathbb{R}^{d \times d_k}$.
    Information theory supplies the loss: cross-entropy measures the
    Kullback&ndash;Leibler divergence between the model&rsquo;s predicted
    distribution and the empirical next-token distribution. Today we follow
    the graph-theory thread end to end, and by the final equation you will
    see exactly where the other two branches enter.
  </aside>
</section>

<!-- ============================================================
     SLIDE 2c — The Mathematical Constellation
     ============================================================ -->
<section class="center-layout">
  <h2>The Mathematical Universe of AI</h2>
  <figure>
    <img src="images/25-math-constellation.png"
         alt="Constellation of mathematical branches feeding into LLMs"
         style="max-height:22vh; max-width:98%;">
  </figure>
  <p class="fragment fade-up text-lg mt-sm">
    Each branch deserves its own lecture. Today we follow <span class="hl-yellow">one thread</span> all the way through.
  </p>

  <aside class="notes">
    This diagram maps the mathematical landscape behind modern AI: graph
    theory, linear algebra, probability, multivariate calculus, information
    theory, and topology. Each branch is necessary; none is sufficient alone.
    Today we isolate graph theory as our thread and trace it from Euler
    through Cayley, Erd&#337;s, PageRank, and finally the attention equation.
    The other branches will appear naturally at the points where the code of
    the universe demands them.
  </aside>
</section>

<!-- ============================================================
     SLIDE 2b — Watch: How LLMs Think
     ============================================================ -->
<section>
  <h2>Watch: How LLMs Think</h2>
  <iframe data-src="https://www.youtube.com/embed/LPZh9BOjkQs?start=0&end=180"
          data-autoplay
          width="1280" height="720"
          allow="autoplay; fullscreen"
          frameborder="0"
          style="max-width:100%; max-height:23vh;">
  </iframe>
  <aside class="notes">
    This 3-minute clip from 3Blue1Brown visualizes next-token prediction.
    Pay attention to the softmax distribution shifting as each token is
    processed &mdash; what you are seeing is the output of a complete
    weighted digraph recomputed at every layer. The attention weights form
    an adjacency matrix; the softmax normalizes each row into a stochastic
    vector. We will formalize this graph-theoretic interpretation precisely
    when we reach the attention equation.
  </aside>
</section>


<!-- ============================================================
     SLIDE 3 — The Question
     ============================================================ -->
<section class="flex-center center-layout">
  <h2 class="text-2xl" style="line-height:1.3;">
    Can you cross all <span class="hl-yellow">seven bridges</span> exactly once?
  </h2>
  <p class="fragment fade-up text-lg hl-teal mt-lg">
    This question invented an entire branch of mathematics.
  </p>
  <figure class="fragment fade-in mt-md" data-fragment-index="1">
    <img src="images/portraits/konigsberg-merian-1652.jpg"
         alt="Historical map of Königsberg showing the seven bridges"
         style="max-height:12vh; border-radius:12px;">
    <figcaption class="text-sm hl-muted">Königsberg (now Kaliningrad), Prussia</figcaption>
  </figure>

  <aside class="notes">
    K&ouml;nigsberg, 1736: four landmasses, seven bridges, and a question no
    one could settle empirically. Can you traverse every bridge exactly once?
    Exhaustive search over all possible routes is combinatorially prohibitive.
    Euler&rsquo;s breakthrough was to prove impossibility by reduction to an
    abstract structure &mdash; the first recorded use of what we now call a
    graph. This is where the code of the universe begins its graph-theory
    chapter.
  </aside>
</section>

<!-- ============================================================
     SLIDE 3b — Watch: The Bridge Problem
     ============================================================ -->
<section>
  <h2>Watch: The Bridge Problem</h2>
  <iframe data-src="https://www.youtube.com/embed/nZwSo4vfw6c"
          data-autoplay
          width="1280" height="720"
          allow="autoplay; fullscreen"
          frameborder="0"
          style="max-width:100%; max-height:23vh;">
  </iframe>
  <aside class="notes">
    Let this TED-Ed animation run in full (4 min 19 sec). It covers the
    historical context and Euler&rsquo;s abstraction. When it finishes, we
    formalize the key idea: Euler reduced a geographic problem to a multigraph
    with four vertices and seven edges, then derived a necessary condition on
    vertex degrees. That step &mdash; stripping away geometry to expose
    combinatorial structure &mdash; is the founding move of graph theory.
  </aside>
</section>


<!-- ============================================================
     SLIDE 4 — Konigsberg Map
     ============================================================ -->
<section>
  <h2>Konigsberg, 1736
    <span class="milestone-badge milestone-origin">Origin</span>
  </h2>
  <figure>
    <img src="images/01-konigsberg-map.png" alt="Map of Konigsberg showing four landmasses and seven bridges">
    <figcaption>Konigsberg (now Kaliningrad, Russia), 1736</figcaption>
  </figure>

  <aside class="notes">
    The Pregel River divides K&ouml;nigsberg into four landmasses connected by
    seven bridges. Modeled as a multigraph $G = (V, E)$ with $|V| = 4$ and
    $|E| = 7$. The degree sequence is $(5, 3, 3, 3)$ &mdash; all vertices
    have odd degree. No brute-force enumeration of routes could resolve the
    question; what was needed was a structural argument. Euler supplied exactly
    that, and in doing so created the first theorem in graph theory.
  </aside>
</section>


<!-- ============================================================
     SLIDE 5 — Euler's Insight (Two-Column)
     ============================================================ -->
<section>
  <h2>Euler's Insight</h2>
  <div class="columns">
    <div class="fragment fade-in" data-fragment-index="1">
      <img src="images/01-konigsberg-map.png" alt="Konigsberg map" style="max-height:22vh;">
      <p class="text-sm hl-muted text-center">The map</p>
    </div>
    <div class="fragment fade-in" data-fragment-index="2">
      <img src="images/02-konigsberg-graph.png" alt="Konigsberg graph abstraction" style="max-height:22vh;">
      <p class="text-sm hl-muted text-center">The graph</p>
    </div>
  </div>
  <p class="fragment fade-up big-quote" data-fragment-index="3" style="font-size:1.15em;">
    "The shape doesn't matter. Only the connections do."
  </p>

  <aside class="notes">
    Euler&rsquo;s decisive move was abstraction: discard metric information
    (distances, angles, curvature) and retain only incidence structure. Landmasses
    become vertices; bridges become edges. This is the same move that later
    gave rise to topology &mdash; Euler himself laid groundwork for both fields.
    The K&ouml;nigsberg problem is often cited as the origin of both graph theory
    and combinatorial topology. Here the mathematical thread connects the
    study of discrete structures to the study of continuous ones.
  </aside>
</section>


<!-- ============================================================
     SLIDE 6 — Graph Vocabulary
     ============================================================ -->
<section>
  <h2>Graph Vocabulary</h2>
  <div class="columns-4060">
    <div>
      <img src="images/02-konigsberg-graph.png" alt="Konigsberg graph with labeled nodes" style="max-height:20vh;">
    </div>
    <div>
      <div class="card fragment fade-in" data-fragment-index="1">
        <h4><span class="def-term">Vertex / Node</span></h4>
        <p class="text-sm mb-0">A point in the graph &mdash; a thing.</p>
      </div>
      <div class="card card-teal fragment fade-in" data-fragment-index="2">
        <h4><span class="def-term" style="color:var(--teal);">Edge</span></h4>
        <p class="text-sm mb-0">A connection between two nodes &mdash; a relationship.</p>
      </div>
      <div class="card card-green fragment fade-in" data-fragment-index="3">
        <h4><span class="def-term" style="color:var(--green);">Degree</span></h4>
        <p class="text-sm mb-0">How many edges touch a node. In Konigsberg: all nodes have odd degree.</p>
      </div>
    </div>
  </div>

  <aside class="notes">
    Formally: a graph $G = (V, E)$ with vertex set $V$ and edge set
    $E \subseteq \binom{V}{2}$ (or a multiset for multigraphs like K&ouml;nigsberg).
    The degree $\deg(v)$ counts edges incident to $v$. Here the degree sequence
    is $(5, 3, 3, 3)$. By the Handshaking Lemma, $\sum_v \deg(v) = 2|E| = 14$,
    so the number of odd-degree vertices is necessarily even. K&ouml;nigsberg
    has four such vertices &mdash; already a signal that Euler&rsquo;s theorem
    will yield impossibility.
  </aside>
</section>


<!-- ============================================================
     SLIDE 7 — Euler's Answer (with vertical proof)
     ============================================================ -->
<section>
  <!-- Main Euler's Theorem slide -->
  <section>
    <h2>Euler's Theorem
      <span class="milestone-badge milestone-discovery">Discovery</span>
    </h2>
    <div class="fragment fade-in" data-fragment-index="1">
      <img src="images/03-euler-path-rule.png" alt="Euler path rule with degree annotations" style="max-height:20vh;">
    </div>
    <div class="formula-box fragment fade-in" data-fragment-index="2">
      <span class="formula-label">Euler's Criterion</span>
      <p>An Eulerian path exists if and only if the graph has exactly <strong>0 or 2</strong> vertices of odd degree.</p>
    </div>
    <div class="callout callout-orange fragment fade-up" data-fragment-index="3">
      <p class="mb-0">Konigsberg has <strong>4</strong> odd-degree vertices. Need 0 or 2. <span class="hl-red">Impossible.</span></p>
    </div>
    <p class="text-sm hl-muted fragment fade-up mt-sm" data-fragment-index="4">&darr; Press down for the proof</p>

    <aside class="notes">
      Euler&rsquo;s theorem: a connected multigraph admits an Eulerian circuit
      iff every vertex has even degree; it admits an Eulerian path (non-closed)
      iff exactly two vertices have odd degree. K&ouml;nigsberg has four
      odd-degree vertices, so neither exists. The proof uses only the
      Handshaking Lemma and a parity argument &mdash; press down for the
      details. This is the first impossibility proof in combinatorics and the
      first theorem of graph theory.
    </aside>
  </section>

  <!-- Vertical: Proof -->
  <section>
    <h3>Why It Works: Proof of Euler's Criterion</h3>
    <div class="card">
      <p><strong>Handshaking Lemma:</strong> $\sum_{v \in V} \deg(v) = 2|E|$</p>
      <p class="text-sm mb-0 hl-muted">Every edge contributes 2 to the total degree. So the number of odd-degree vertices is always even.</p>
    </div>
    <ol class="fragment fade-in text-sm" style="line-height:1.5;">
      <li>Traversing an edge uses one &ldquo;entry&rdquo; and one &ldquo;exit&rdquo; at each interior vertex &mdash; consuming 2 edges per visit.</li>
      <li>Interior vertices must have <strong>even degree</strong> (each visit uses a pair).</li>
      <li>Start/end vertices may have odd degree (one unpaired entry or exit).</li>
      <li>Therefore: <strong>0 odd vertices</strong> (circuit) or <strong>2 odd vertices</strong> (path). &check;</li>
    </ol>

    <aside class="notes">
      The Handshaking Lemma ($\sum_v \deg(v) = 2|E|$) implies the number of
      odd-degree vertices is even. For the Eulerian criterion: each transit
      through an interior vertex consumes one entering edge and one leaving
      edge &mdash; a matched pair &mdash; so interior vertices require even
      degree. Only the endpoints of the trail may have odd degree (one unmatched
      edge each). Hence: 0 odd-degree vertices yields a circuit, 2 yields a
      trail, and any other even count (like 4) yields impossibility. The
      sufficiency direction uses a constructive algorithm (Hierholzer&rsquo;s,
      1873). Euler stated necessity; the full iff was completed later.
    </aside>
  </section>
</section>


<!-- ============================================================
     SLIDE 8 — Euler's Legacy (Big Quote)
     ============================================================ -->
<section>
  <div class="columns-4060">
    <div class="col-center" style="text-align:center;">
      <img src="images/portraits/euler-handmann-1753.jpg"
           alt="Leonhard Euler, portrait by Jakob Emanuel Handmann, 1753"
           style="max-height:18vh; border-radius:12px; box-shadow: 0 4px 20px rgba(0,0,0,0.5);">
      <p class="text-sm hl-muted" style="margin-top:0.5rem;">Leonhard Euler (1707–1783)</p>
    </div>
    <div class="col-center">
      <div class="big-quote" style="font-size:0.9em;">
        "This solution bore little relationship to mathematics&hellip; yet I do not
        know why you are not satisfied with it."
        <span class="attribution">&mdash; Leonhard Euler, 1736</span>
      </div>
      <p class="fragment fade-up mt-lg">
        Euler didn't just solve a puzzle. He invented a <strong>new way of thinking</strong> about structure.
      </p>
    </div>
  </div>

  <aside class="notes">
    Euler himself classified this as &ldquo;geometria situs&rdquo; &mdash;
    geometry of position &mdash; distinct from the calculus and algebra of his
    day. That concept became the seed of topology: the study of properties
    invariant under continuous deformation. Euler&rsquo;s polyhedron formula
    $V - E + F = 2$ (1758) is another line of the same code, linking graph
    theory to topological invariants. His K&ouml;nigsberg proof inaugurated
    the principle that structure, not magnitude, can be the decisive
    mathematical quantity.
  </aside>
</section>


<!-- ============================================================
     SLIDE 9 — Cayley's Trees (1857)
     ============================================================ -->
<section>
  <h2>Cayley's Trees (1857)
    <span class="milestone-badge milestone-discovery">Discovery</span>
  </h2>
  <p class="text-lg">How many different tree-shaped graphs can you make with $n$ labeled nodes?</p>
  <div class="columns-4060 mt-md">
    <div style="text-align:center;">
      <img src="images/portraits/cayley-engraving-1883.jpg"
           alt="Arthur Cayley, engraving 1883"
           style="max-height:13vh; border-radius:12px; box-shadow: 0 4px 20px rgba(0,0,0,0.5);">
      <p class="text-sm hl-muted" style="margin-top:0.3rem;">Arthur Cayley (1821–1895)</p>
    </div>
    <figure class="fragment fade-in" data-fragment-index="1">
      <img src="images/04-cayley-trees.png" alt="All labeled trees on 4 nodes">
      <figcaption>All 16 labeled trees on 4 nodes</figcaption>
    </figure>
  </div>

  <aside class="notes">
    Cayley posed a fundamental enumerative question: how many spanning trees
    exist on $n$ labeled vertices? A tree on $n$ vertices has exactly $n-1$
    edges and is the unique minimal connected graph on those vertices. For
    $n = 3$: three trees. For $n = 4$: sixteen. The growth rate is
    super-exponential, and the closed form is not obvious from small cases.
    This is another line of the code &mdash; combinatorics meeting graph
    structure.
  </aside>
</section>


<!-- ============================================================
     SLIDE 10 — Cayley's Formula (with vertical deep-dive)
     ============================================================ -->
<section>
  <!-- Main slide -->
  <section>
    <h2>Cayley's Formula</h2>
    <div class="formula-box">
      <span class="formula-label">Number of Labeled Trees</span>
      $$T_n = n^{n-2}$$
    </div>
    <div class="columns-3 mt-lg">
      <div class="card fragment fade-in" data-fragment-index="1">
        <p class="stat-number">3</p>
        <p class="stat-label">$T_3 = 3^1$</p>
      </div>
      <div class="card card-teal fragment fade-in" data-fragment-index="2">
        <p class="stat-number" style="color:var(--teal);">16</p>
        <p class="stat-label">$T_4 = 4^2$</p>
      </div>
      <div class="card card-yellow fragment fade-in" data-fragment-index="3">
        <p class="stat-number" style="color:var(--yellow);">10<sup style="font-size:0.5em;">8</sup></p>
        <p class="stat-label">$T_{10} = 10^8$</p>
      </div>
    </div>
    <p class="text-sm hl-muted fragment fade-up mt-md" data-fragment-index="4">&darr; Press down for proof sketch</p>

    <aside class="notes">
      Cayley&rsquo;s formula: $T_n = n^{n-2}$. The Pr&uuml;fer bijection gives
      an elementary proof &mdash; it establishes a bijection between labeled
      trees on $[n]$ and sequences of length $n-2$ over the alphabet $[n]$,
      so the count is $n^{n-2}$. The growth is super-exponential: $T_{10} = 10^8$.
      Press down for the Pr&uuml;fer construction and for Kirchhoff&rsquo;s
      Matrix-Tree Theorem, which recovers the same count via linear algebra.
    </aside>
  </section>

  <!-- Vertical sub-slide: Prufer sequence proof sketch -->
  <section>
    <h3>Proof Sketch: Prufer Sequences</h3>
    <div class="card">
      <p>A <span class="def-term">Prufer sequence</span> is a bijection between labeled trees on $n$ nodes and sequences of length $n-2$ drawn from $\{1, 2, \ldots, n\}$.</p>
    </div>
    <ol class="fragment fade-in">
      <li>Repeatedly remove the leaf with the smallest label; record the label of its neighbor.</li>
      <li>This yields a sequence of $n - 2$ numbers, each in $\{1, \ldots, n\}$.</li>
      <li>The map is reversible: every sequence corresponds to exactly one tree.</li>
      <li>There are $n^{n-2}$ such sequences &rarr; $T_n = n^{n-2}$. &check;</li>
    </ol>

    <aside class="notes">
      The Pr&uuml;fer bijection: iteratively remove the leaf with the smallest
      label and record its neighbor. This encodes any labeled tree on $[n]$ as a
      unique sequence in $[n]^{n-2}$, and the inverse map reconstructs the tree
      from the sequence. Both directions run in $O(n \log n)$. Since there are
      $n^{n-2}$ such sequences, Cayley&rsquo;s formula follows immediately.
      The proof is constructive, combinatorial, and entirely elementary &mdash;
      no algebra or calculus required.
    </aside>
  </section>

  <!-- Vertical sub-slide: Kirchhoff's Matrix-Tree Theorem -->
  <section>
    <h3>The Matrix-Tree Theorem (Kirchhoff, 1847)</h3>
    <div class="formula-box">
      <span class="formula-label">Graph Laplacian</span>
      $$L = D - A$$
    </div>
    <div class="columns mt-sm">
      <div class="card text-sm">
        <p class="mb-0">$D$ = degree matrix (diagonal)<br>$A$ = adjacency matrix</p>
      </div>
      <div class="card card-teal text-sm">
        <p class="mb-0">The number of spanning trees = any cofactor of $L$</p>
      </div>
    </div>
    <p class="fragment fade-up text-sm hl-muted mt-sm">
      This connects trees to <strong>linear algebra</strong> &mdash; the Laplacian reappears in PageRank, GNNs, and spectral clustering.
    </p>

    <aside class="notes">
      Kirchhoff&rsquo;s Matrix-Tree Theorem (1847): the number of spanning trees
      of $G$ equals any cofactor of the graph Laplacian $L = D - A$. For the
      complete graph $K_n$, the Laplacian has eigenvalues $0$ (once) and $n$
      (with multiplicity $n-1$), so the cofactor is $n^{n-2}$ &mdash; recovering
      Cayley&rsquo;s formula. Here the mathematical thread connects combinatorics
      to linear algebra. The Laplacian reappears in spectral clustering, in
      PageRank, and in the propagation rule of graph neural networks. It is one
      of the most important matrices in all of applied mathematics.
    </aside>
  </section>
</section>


<!-- ============================================================
     SLIDE 11 — Trees in Computer Science
     ============================================================ -->
<section>
  <h2>Trees in Computer Science</h2>
  <div class="columns">
    <div>
      <img src="images/05-parse-tree.png" alt="Parse tree diagram" style="max-height:21vh;">
      <p class="text-sm hl-muted text-center">Parse tree: how a compiler reads your code</p>
    </div>
    <div class="col-center">
      <div class="card fragment fade-in" data-fragment-index="1">
        <h4>Parse Trees</h4>
        <p class="text-sm mb-0">Compilers break code into tree structures to understand syntax.</p>
      </div>
      <div class="card card-teal fragment fade-in" data-fragment-index="2">
        <h4>Decision Trees</h4>
        <p class="text-sm mb-0">ML models that split data by asking yes/no questions at each node.</p>
      </div>
      <div class="card card-yellow fragment fade-in" data-fragment-index="3">
        <h4>File Systems</h4>
        <p class="text-sm mb-0">Your folders and files form a tree rooted at <code>/</code> or <code>C:\</code>.</p>
      </div>
    </div>
  </div>

  <aside class="notes">
    Trees are the canonical data structure in computer science. Compilers parse
    source code into abstract syntax trees (ASTs). Balanced binary search trees
    give $O(\log n)$ lookup, insertion, and deletion. Decision trees partition
    feature space by recursive binary splits &mdash; the basis of random forests
    and gradient-boosted methods like XGBoost. File systems are rooted trees.
    The DOM is a tree. Cayley&rsquo;s 1857 abstraction permeates every layer
    of the computational stack, from hardware routing trees to ML model
    architectures.
  </aside>
</section>


<!-- ============================================================
     SLIDE 12 — Erdos-Renyi (1959)
     ============================================================ -->
<section>
  <h2>Erdos &amp; Renyi (1959)
    <span class="milestone-badge milestone-breakthrough">Breakthrough</span>
  </h2>
  <p class="text-lg">What happens when you <strong>randomly</strong> connect nodes?</p>
  <div class="columns-4060 mt-md">
    <div style="text-align:center;">
      <img src="images/portraits/erdos-portrait.jpg"
           alt="Paul Erdős"
           style="max-height:13vh; border-radius:12px; box-shadow: 0 4px 20px rgba(0,0,0,0.5);">
      <p class="text-sm hl-muted" style="margin-top:0.5rem;">Paul Erdős (1913–1996)</p>
    </div>
    <div class="card card-orange fragment fade-up">
      <h4 style="color:var(--orange);">The Most Prolific Mathematician</h4>
      <p class="text-sm mb-0">
        <strong>1,500+ papers</strong>, 500+ collaborators.
        He had no permanent home &mdash; just a suitcase and a
        desire to do mathematics everywhere.
      </p>
    </div>
  </div>
  <p class="fragment fade-up text-sm hl-muted mt-md">
    With Alfred Renyi, he asked: if we connect $n$ nodes randomly, each edge appearing with probability $p$, what structure emerges?
  </p>

  <aside class="notes">
    Erd&#337;s published over 1,500 papers with 500+ co-authors &mdash; the
    highest Erd&#337;s number is still a measure of collaborative distance in
    mathematics. With R&eacute;nyi, he introduced the random graph model
    $G(n, p)$: start with $n$ isolated vertices and include each of the
    $\binom{n}{2}$ possible edges independently with probability $p$. The
    central question: as $p$ increases from 0 to 1, when does qualitative
    structure emerge? The answer involves a sharp phase transition that we
    examine on the next slide.
  </aside>
</section>


<!-- ============================================================
     SLIDE 13 — Random Graph Phase Transition
     ============================================================ -->
<section>
  <h2>Phase Transition</h2>
  <figure>
    <img src="images/06-random-graph-phases.png" alt="Four phases of random graph evolution">
  </figure>
  <div class="columns-3 mt-sm">
    <p class="fragment fade-in text-sm text-center" data-fragment-index="1">
      <span class="hl-blue">Sparse:</span> isolated clusters
    </p>
    <p class="fragment fade-in text-sm text-center" data-fragment-index="2">
      <span class="hl-yellow">Critical:</span> giant component forms
    </p>
    <p class="fragment fade-in text-sm text-center" data-fragment-index="3">
      <span class="hl-green">Dense:</span> almost everything connected
    </p>
  </div>
  <div class="formula-box fragment fade-in mt-sm" data-fragment-index="4">
    <span class="formula-label">Critical Threshold</span>
    <p class="mb-0">$G(n,p)$ with $p = c/n$: giant component emerges when $c > 1$ (average degree $> 1$)</p>
  </div>

  <aside class="notes">
    The precise threshold statement: in $G(n, p)$ with $p = c/n$, for $c &lt; 1$
    the largest component has size $O(\log n)$ almost surely. At $c = 1$ the
    largest component is $\Theta(n^{2/3})$. For $c &gt; 1$ a unique giant
    component of size $\Theta(n)$ emerges, containing a fraction
    $1 - T(c)/c$ of all vertices, where $T$ is the tree function satisfying
    $T = ce^{-T}$. This is a genuine phase transition &mdash; a discontinuity
    in the order parameter analogous to percolation on a lattice. The
    mathematical thread here connects discrete probability to statistical
    physics.
  </aside>
</section>


<!-- ============================================================
     SLIDE 14 — Emergence: Graphs to AI
     ============================================================ -->
<section>
  <h2>Emergence: Graphs &rarr; AI
    <span class="milestone-badge milestone-ai-connection">AI Connection</span>
  </h2>
  <div class="columns">
    <div class="card">
      <h4>Random Graphs</h4>
      <p class="text-sm">At a critical edge density, structure <strong>suddenly</strong> appears.</p>
      <p class="text-sm mb-0 hl-muted">Small change in $p$ &rarr; qualitative leap.</p>
    </div>
    <div class="card card-yellow">
      <h4 style="color:var(--yellow);">Large Language Models</h4>
      <p class="text-sm">At a critical parameter count, new abilities <strong>suddenly</strong> emerge.</p>
      <p class="text-sm mb-0 hl-muted">More parameters &rarr; reasoning, code, math.</p>
    </div>
  </div>
  <figure class="mt-sm" style="text-align:center;">
    <img src="images/24-emergence-phase-transition.png"
         alt="Dual panel showing LLM emergence sigmoid curves alongside Erdos-Renyi phase transition — same mathematical pattern"
         style="max-height:16vh; max-width:98%;">
    <figcaption class="text-sm hl-muted">Same math, different systems</figcaption>
  </figure>

  <aside class="notes">
    The Erd&#337;s&ndash;R&eacute;nyi phase transition has a direct analogue in
    deep learning. Kaplan et al.&rsquo;s scaling laws (2020) show that test loss
    follows a power law in model size, but certain capabilities &mdash;
    arithmetic, chain-of-thought reasoning, code generation &mdash; emerge
    abruptly past critical parameter counts, resembling a sigmoid step
    function rather than gradual improvement. Whether this constitutes a true
    phase transition in the statistical-physics sense is an active research
    question, but the phenomenology mirrors graph percolation: a quantitative
    increase triggers a qualitative leap.
  </aside>
</section>


<!-- ============================================================
     SLIDE 15 — Act I Recap
     ============================================================ -->
<section>
  <h2>Act I Recap: Foundations</h2>
  <div class="summary-cards">
    <div class="summary-card fragment fade-in" data-fragment-index="1">
      <span class="card-icon">1736</span>
      <h4>Euler</h4>
      <p>Turned a bridge puzzle into a new branch of mathematics. Connections &gt; things.</p>
    </div>
    <div class="summary-card fragment fade-in" data-fragment-index="2">
      <span class="card-icon">1857</span>
      <h4>Cayley</h4>
      <p>Counted trees: $T_n = n^{n-2}$. Combinatorics meets graph structure.</p>
    </div>
    <div class="summary-card fragment fade-in" data-fragment-index="3">
      <span class="card-icon">1959</span>
      <h4>Erdos &amp; Renyi</h4>
      <p>Random graphs undergo phase transitions &mdash; structure from chaos.</p>
    </div>
  </div>

  <aside class="notes">
    Three foundational results across two centuries. Euler (1736): abstraction
    to incidence structure and the first impossibility proof via degree parity.
    Cayley (1857): the enumeration $T_n = n^{n-2}$ via the Pr&uuml;fer
    bijection, later deepened by Kirchhoff&rsquo;s spectral approach. Erd&#337;s
    &amp; R&eacute;nyi (1959): a sharp phase transition in $G(n, c/n)$ at
    $c = 1$. Each result layers onto the last, building the combinatorial and
    probabilistic toolkit that Act II will apply to real-world networks.
  </aside>
</section>


<!-- ============================================================
     SLIDE 16 — Act II Title (Section Divider)
     ============================================================ -->
<section>
  <div class="section-divider">
    <p class="act-label">Act II</p>
    <h2>The World Is a Graph</h2>
    <p class="act-subtitle">Small worlds, big data, and a company worth trillions</p>
  </div>

  <aside class="notes">
    Act II bridges pure mathematics and applied network science. The internet
    is a directed graph with billions of vertices. Social networks are
    undirected graphs with power-law degree distributions. The key question
    shifts from existence and enumeration to structure and computation: given
    a massive real-world graph, what can we efficiently extract from it?
  </aside>
</section>


<!-- ============================================================
     SLIDE 17 — Six Degrees of Separation
     ============================================================ -->
<section class="center-layout">
  <h2>Six Degrees of Separation</h2>
  <p class="text-xl mt-md">
    How many people separate you from <strong>any other person</strong> on Earth?
  </p>
  <p class="fragment fade-up stat-number mt-lg" data-fragment-index="1" style="font-size:4em;">~6</p>
  <p class="fragment fade-up hl-muted" data-fragment-index="1">Stanley Milgram's letter experiment, 1967</p>
  <figure class="fragment fade-in mt-md" data-fragment-index="2">
    <img src="images/18-milgram-letters.png"
         alt="Chain of letters from Nebraska to Boston through 6 intermediaries"
         style="max-height:10vh;">
  </figure>

  <aside class="notes">
    Milgram&rsquo;s 1967 experiment measured average path length in the social
    graph: letters forwarded person-to-person from Nebraska to a target in
    Boston required a median of about 6 intermediaries. Distinguish this from
    the graph diameter, which is the maximum shortest-path distance &mdash;
    necessarily larger. Facebook&rsquo;s 2016 study of 1.59 billion users found
    an average path length of 3.57 and an estimated diameter around 41. The
    &ldquo;six degrees&rdquo; phenomenon refers to the average, not the worst
    case. This is another line of the code &mdash; the small-world property
    of real networks.
  </aside>
</section>


<!-- ============================================================
     SLIDE 18 — Small World Networks
     ============================================================ -->
<section>
  <h2>Small-World Networks (1998)
    <span class="milestone-badge milestone-breakthrough">Breakthrough</span>
  </h2>
  <figure>
    <img src="images/07-small-world.png" alt="Three panels: regular, small-world, random network">
  </figure>
  <div class="callout fragment fade-up mt-sm">
    <p class="mb-0"><strong>Small-world property:</strong> High clustering (friends of friends are friends) + short average path length (anyone reachable in few hops).</p>
  </div>

  <aside class="notes">
    Watts &amp; Strogatz (1998) formalized the small-world property with two
    metrics: high clustering coefficient $C$ (probability that two neighbors
    of a vertex are themselves adjacent) and low average shortest-path length
    $L$ (scaling as $O(\log n)$). Their model starts from a ring lattice and
    rewires each edge with probability $p$. Even a small $p$ collapses $L$
    toward the random-graph value while $C$ remains near the lattice value.
    Empirical examples include C. elegans neural wiring ($C = 0.28$,
    $L = 2.65$) and the Western US power grid. The small-world property
    explains why Milgram&rsquo;s letters needed so few hops.
  </aside>
</section>


<!-- ============================================================
     SLIDE 19 — Small Worlds in AI
     ============================================================ -->
<section>
  <h2>Small Worlds in AI
    <span class="milestone-badge milestone-ai-connection">AI Connection</span>
  </h2>
  <p>Skip connections in <strong>ResNets</strong> and <strong>Transformers</strong> create small-world shortcuts.</p>
  <div class="columns mt-md">
    <div class="card fragment fade-in" data-fragment-index="1">
      <h4>Without Skip Connections</h4>
      <p class="text-sm">Information must travel through every layer sequentially. Gradients vanish over long paths.</p>
      <p class="text-sm mb-0 hl-muted">Like a regular lattice &mdash; long path length.</p>
    </div>
    <div class="card card-green fragment fade-in" data-fragment-index="2">
      <h4 style="color:var(--green);">With Skip Connections</h4>
      <p class="text-sm">Information can jump across layers. Gradients flow freely.</p>
      <p class="text-sm mb-0 hl-muted">Like a small-world network &mdash; short paths!</p>
    </div>
  </div>

  <aside class="notes">
    ResNets (He et al., 2015) introduced skip connections: identity mappings
    that bypass one or more layers. From a graph-theory perspective, these are
    long-range edges that reduce the effective diameter of the computational
    graph, mirroring the Watts&ndash;Strogatz rewiring. Without them, gradients
    attenuate exponentially with depth (the vanishing gradient problem).
    Transformers take this further: self-attention gives every token a direct
    edge to every other token, making the computational graph a complete
    digraph &mdash; diameter 1. Here the mathematical thread connects network
    science to deep learning architecture design.
  </aside>
</section>


<!-- ============================================================
     SLIDE 20 — INTERACTIVE: Six Degrees
     ============================================================ -->
<section>
  <h2>Interactive: Six Degrees Path Tracer</h2>
  <p class="text-sm hl-muted">Click nodes to trace a path from <strong>You</strong> to <strong>Celebrity</strong>. How few steps can you find?</p>
  <div id="interactive-six-degrees" class="interactive-container tall"></div>

  <aside class="notes">
    This interactive demonstrates BFS shortest-path computation on a small
    graph. Starting from &ldquo;You,&rdquo; click adjacent vertices to extend
    your path toward &ldquo;Celebrity.&rdquo; The &ldquo;Show Shortest&rdquo;
    button runs BFS and displays the optimal path. In this graph, the shortest
    path is typically 3&ndash;4 edges &mdash; illustrating the small-world
    property even in a network with only a dozen vertices. BFS finds shortest
    paths in $O(|V| + |E|)$ time.
  </aside>
</section>


<!-- ============================================================
     SLIDE 21 — PageRank: Google's Graph
     ============================================================ -->
<section>
  <h2>PageRank: Google's Graph
    <span class="milestone-badge milestone-breakthrough">Breakthrough</span>
  </h2>
  <p class="text-lg">
    <strong>1998:</strong> Two Stanford students turned the entire internet into a graph &mdash;
    and created a <span class="hl-yellow">trillion-dollar company</span>.
  </p>
  <div class="columns mt-md">
    <div style="display:flex; gap:1rem; justify-content:center; align-items:flex-end;">
      <div style="text-align:center;">
        <img src="images/portraits/larry-page.jpg"
             alt="Larry Page"
             style="max-height:12vh; border-radius:12px; box-shadow: 0 4px 20px rgba(0,0,0,0.5);">
        <p class="text-sm hl-muted" style="margin-top:0.3rem;">Larry Page</p>
      </div>
      <div style="text-align:center;">
        <img src="images/portraits/sergey-brin.jpg"
             alt="Sergey Brin"
             style="max-height:12vh; border-radius:12px; box-shadow: 0 4px 20px rgba(0,0,0,0.5);">
        <p class="text-sm hl-muted" style="margin-top:0.3rem;">Sergey Brin</p>
      </div>
    </div>
    <div class="card fragment fade-up col-center">
      <p class="text-sm mb-0">
        <strong>Larry Page</strong> and <strong>Sergey Brin</strong> realized that the web is a
        directed graph: pages are nodes, hyperlinks are edges. The question was not
        "what does this page say?" but "who links to it, and how important are they?"
      </p>
    </div>
  </div>

  <aside class="notes">
    Page and Brin modeled the web as a directed graph $G = (V, E)$ where
    $V$ is the set of web pages and $(u, v) \in E$ iff page $u$ hyperlinks
    to page $v$. Their key insight: importance is not intrinsic to a page
    but determined by the graph&rsquo;s link structure. A page is important
    if it receives edges from other important pages &mdash; a recursive
    definition that linear algebra resolves via eigenvector computation.
    That idea, formalized as PageRank, became the foundation of a
    trillion-dollar company.
  </aside>
</section>


<!-- ============================================================
     SLIDE 22 — How PageRank Works
     ============================================================ -->
<section>
  <h2>How PageRank Works</h2>
  <div class="columns-4060">
    <div>
      <img src="images/09-pagerank-web.png" alt="Web pages as a directed graph" style="max-height:22vh;">
    </div>
    <div>
      <ul class="step-list">
        <li class="fragment fade-in" data-fragment-index="1">
          <div class="step-body">
            <span class="step-title">Pages = Nodes</span>
            <span class="step-desc">Every web page is a vertex in a giant directed graph.</span>
          </div>
        </li>
        <li class="fragment fade-in" data-fragment-index="2">
          <div class="step-body">
            <span class="step-title">Links = Votes</span>
            <span class="step-desc">A link from page A to page B is a "vote" for B's importance.</span>
          </div>
        </li>
        <li class="fragment fade-in" data-fragment-index="3">
          <div class="step-body">
            <span class="step-title">Recursive Importance</span>
            <span class="step-desc">A page is important if important pages link to it.</span>
          </div>
        </li>
        <li class="fragment fade-in" data-fragment-index="4">
          <div class="step-body">
            <span class="step-title">Random Surfer</span>
            <span class="step-desc">Imagine clicking links at random. Where do you end up most often?</span>
          </div>
        </li>
      </ul>
    </div>
  </div>

  <aside class="notes">
    The recursive definition &mdash; a page is important if important pages
    link to it &mdash; is resolved by formulating the web&rsquo;s link
    structure as a column-stochastic matrix $M$. Each column $j$ distributes
    node $j$&rsquo;s rank uniformly among its out-neighbors. The PageRank
    vector is the stationary distribution of the Markov chain defined by the
    &ldquo;random surfer&rdquo;: with probability $d$ follow a random outlink,
    with probability $1-d$ teleport uniformly. The teleportation term ensures
    $M$ is irreducible and aperiodic &mdash; the conditions for the
    Perron&ndash;Frobenius theorem to guarantee a unique stationary distribution.
  </aside>
</section>


<!-- ============================================================
     SLIDE 23 — The PageRank Formula (with vertical deep-dive)
     ============================================================ -->
<section>
  <!-- Main slide -->
  <section>
    <h2>The PageRank Formula</h2>
    <div class="formula-box">
      <span class="formula-label">PageRank Equation</span>
      $$PR(p) = \frac{1-d}{N} + d \sum_{q \in B_p} \frac{PR(q)}{L(q)}$$
    </div>
    <div class="columns mt-md">
      <div class="card card-orange fragment fade-in" data-fragment-index="1">
        <h4 style="color:var(--orange);">$\frac{1-d}{N}$</h4>
        <p class="text-sm mb-0">Teleportation: with probability $1-d$, jump to a random page.</p>
      </div>
      <div class="card fragment fade-in" data-fragment-index="2">
        <h4>$d \sum \frac{PR(q)}{L(q)}$</h4>
        <p class="text-sm mb-0">Link-following: with probability $d$, follow a random outgoing link.</p>
      </div>
    </div>
    <p class="text-sm hl-muted fragment fade-up mt-md" data-fragment-index="3">&darr; Press down for the Markov chain interpretation</p>

    <aside class="notes">
      Parse the formula term by term. The $(1-d)/N$ term is a uniform
      teleportation probability &mdash; it makes the transition matrix
      primitive (positive entries everywhere). The damping factor $d$ is
      typically 0.85. The summation $\sum_{q \in B_p} PR(q)/L(q)$ aggregates
      rank from each in-neighbor $q$, divided by $q$&rsquo;s out-degree
      $L(q)$ &mdash; this is the column-stochastic contribution. Together,
      the formula defines a convex combination: with probability $d$ follow
      a link, with probability $1-d$ teleport. Convergence of the power
      iteration is guaranteed by Perron&ndash;Frobenius because the matrix
      is stochastic and primitive.
    </aside>
  </section>

  <!-- Vertical sub-slide: Markov chain -->
  <section>
    <h3>PageRank as a Markov Chain</h3>
    <div class="card">
      <p class="text-sm">The random surfer defines a <span class="def-term">Markov chain</span> on the web graph.</p>
    </div>
    <ul class="fragment fade-in">
      <li>Transition matrix $M$: $M_{ij} = d/L(j)$ if $j \to i$, plus $\frac{1-d}{N}$ everywhere.</li>
      <li>PageRank vector $\vec{r}$ satisfies $\vec{r} = M\vec{r}$ &mdash; it is the <strong>dominant eigenvector</strong>.</li>
      <li>Compute by iterating: start with uniform $\vec{r}$, multiply by $M$, repeat until convergence.</li>
      <li>The Perron&ndash;Frobenius theorem guarantees convergence.</li>
    </ul>
    <p class="fragment fade-up text-sm hl-muted mt-md">
      Google's original algorithm did roughly 50&ndash;100 iterations over the entire web graph.
    </p>

    <aside class="notes">
      Formally, the PageRank vector $\vec{r}$ is the dominant left eigenvector
      of the stochastic matrix $M = dS + (1-d)\frac{1}{N}\mathbf{1}\mathbf{1}^T$,
      where $S$ is the column-normalized adjacency matrix. The
      Perron&ndash;Frobenius theorem guarantees a unique eigenvector with
      eigenvalue 1 because $M$ is irreducible (the teleportation term connects
      every pair) and aperiodic (self-loops via teleportation). The power
      method converges geometrically at rate $d$ &mdash; with $d = 0.85$,
      roughly 50&ndash;100 iterations suffice. Google ran this on a matrix
      with billions of nonzero entries.
    </aside>
  </section>

  <!-- Vertical: Worked 4-node Example -->
  <section>
    <h3>Worked Example: 4-Node PageRank</h3>
    <p class="text-sm">Transition matrix $M$ and power iteration $\vec{r}_{t+1} = M\,\vec{r}_t$:</p>
    <div class="columns mt-sm">
      <div class="text-sm">
        $$M = \begin{pmatrix} 0 & \frac{1}{2} & 0 & 0 \\ \frac{1}{3} & 0 & 0 & \frac{1}{2} \\ \frac{1}{3} & \frac{1}{2} & 0 & \frac{1}{2} \\ \frac{1}{3} & 0 & 1 & 0 \end{pmatrix}$$
      </div>
      <div>
        <div class="card text-sm">
          <p class="mb-0"><strong>After 1 iteration:</strong> $\vec{r}_1 = (0.21, 0.21, 0.33, 0.25)$</p>
        </div>
        <div class="card card-teal text-sm">
          <p class="mb-0"><strong>Converged:</strong> $\vec{r}_\infty = (0.12, 0.17, 0.31, 0.40)$</p>
        </div>
        <p class="text-sm hl-muted">Node D wins &mdash; it is linked by the highest-ranked node.</p>
      </div>
    </div>

    <aside class="notes">
      This 4-node example illustrates the power method concretely. Column $j$
      of $M$ distributes node $j$&rsquo;s rank uniformly among its out-neighbors.
      Starting from $\vec{r}_0 = (0.25, 0.25, 0.25, 0.25)$, repeated
      multiplication by $M$ converges to the stationary distribution
      $\vec{r}_\infty \approx (0.12, 0.17, 0.31, 0.40)$. Node D dominates
      because it is the sole out-neighbor of node C, which itself has high rank
      &mdash; the recursive structure at work. This is the power method for
      computing the dominant eigenvector of a nonnegative matrix, a fundamental
      algorithm in numerical linear algebra.
    </aside>
  </section>
</section>


<!-- ============================================================
     SLIDE 24 — INTERACTIVE: PageRank Ranking
     ============================================================ -->
<section>
  <h2>Interactive: Rank by PageRank</h2>
  <p class="text-sm hl-muted">Drag the node chips into ranking slots. Who has the most incoming links from important pages?</p>
  <div id="interactive-pagerank" class="interactive-container tall"></div>

  <aside class="notes">
    Rank the five nodes by estimated stationary distribution. The key insight
    is that in-degree alone does not determine PageRank &mdash; the rank of
    the linking nodes matters. A single edge from a high-rank node contributes
    more than many edges from low-rank nodes. Drag the chips, then check
    against the power-method solution. Node C typically dominates because it
    is an in-neighbor of nearly every other vertex in this particular graph.
  </aside>
</section>


<!-- ============================================================
     SLIDE 25 — PageRank → Modern AI
     ============================================================ -->
<section>
  <h2>PageRank &rarr; Modern AI
    <span class="milestone-badge milestone-ai-connection">AI Connection</span>
  </h2>
  <p>Three instances of the same idea: <strong>learn structure from a matrix derived from connections.</strong></p>
  <div class="columns-3 mt-md">
    <div class="card fragment fade-in" data-fragment-index="1">
      <h4>PageRank</h4>
      <p class="text-sm mb-0">Eigenvector of the web graph's transition matrix</p>
      <p class="text-sm mb-0 hl-muted">$$\vec{r} = M \vec{r}$$</p>
    </div>
    <div class="card card-teal fragment fade-in" data-fragment-index="2">
      <h4 style="color:var(--teal);">Word2Vec</h4>
      <p class="text-sm mb-0">Eigenvectors of the word co-occurrence matrix</p>
      <p class="text-sm mb-0 hl-muted">$\text{PMI}(w_i, w_j) = \log \frac{P(w_i, w_j)}{P(w_i)P(w_j)}$</p>
    </div>
    <div class="card card-yellow fragment fade-in" data-fragment-index="3">
      <h4 style="color:var(--yellow);">Attention</h4>
      <p class="text-sm mb-0">Softmax of the token similarity matrix</p>
      <p class="text-sm mb-0 hl-muted">$QK^T / \sqrt{d_k}$</p>
    </div>
  </div>
  <p class="fragment fade-up mt-md hl-teal">
    Three eras, one principle: <strong>learn structure from a matrix derived from connections.</strong>
  </p>

  <aside class="notes">
    Three eras, one principle. PageRank: the dominant eigenvector of a
    stochastic transition matrix derived from the web graph. Word2Vec (2013):
    Levy &amp; Goldberg showed that Skip-gram with negative sampling implicitly
    factorizes the PMI matrix of word co-occurrences &mdash; effectively the
    weighted adjacency matrix of a word-context graph. Attention (2017): the
    softmax of $QK^T/\sqrt{d_k}$ is a row-stochastic matrix &mdash; a
    normalized adjacency matrix of a complete token graph. In each case, the
    algorithm builds a matrix from connections and extracts structure via
    linear-algebraic operations. This is the unifying mathematical thread.
  </aside>
</section>


<!-- ============================================================
     SLIDE 26 — Social Network Analysis
     ============================================================ -->
<section>
  <h2>Social Networks Are Graphs</h2>
  <div class="columns-4060">
    <div>
      <img src="images/08-six-degrees.png" alt="Social network graph" style="max-height:22vh;">
    </div>
    <div>
      <p>Every social media platform runs graph algorithms at scale:</p>
      <ul>
        <li class="fragment fade-in" data-fragment-index="1"><strong>Friend recommendations</strong> &mdash; triangle closure</li>
        <li class="fragment fade-in" data-fragment-index="2"><strong>Influence scoring</strong> &mdash; centrality measures</li>
        <li class="fragment fade-in" data-fragment-index="3"><strong>Community detection</strong> &mdash; graph clustering</li>
        <li class="fragment fade-in" data-fragment-index="4"><strong>Viral spread prediction</strong> &mdash; diffusion on networks</li>
      </ul>
      <p class="fragment fade-up text-sm hl-muted mt-md" data-fragment-index="5">
        Facebook's social graph: 3+ billion nodes, hundreds of billions of edges.
      </p>
    </div>
  </div>

  <aside class="notes">
    Social platforms deploy graph algorithms at planetary scale. Friend
    recommendation exploits triangle closure (triadic closure principle):
    if $\{A, B\}$ and $\{B, C\}$ are edges, propose $\{A, C\}$. Influence
    scoring uses centrality measures &mdash; degree, betweenness, eigenvector
    (of which PageRank is a variant). Community detection algorithms like the
    Louvain method optimize modularity $Q = \frac{1}{2m}\sum_{ij}[A_{ij} -
    \frac{k_i k_j}{2m}]\delta(c_i, c_j)$ to partition the graph into dense
    clusters. Viral-spread models apply SIR/SIS epidemic dynamics on the graph.
    Facebook&rsquo;s graph exceeds 3 billion vertices and hundreds of billions
    of edges.
  </aside>
</section>


<!-- ============================================================
     SLIDE 27 — Act II Recap
     ============================================================ -->
<section>
  <h2>Act II Recap: The Real World</h2>
  <div class="summary-cards">
    <div class="summary-card fragment fade-in" data-fragment-index="1">
      <span class="card-icon">1998</span>
      <h4>Small Worlds</h4>
      <p>High clustering + short paths. Why six degrees works.</p>
    </div>
    <div class="summary-card fragment fade-in" data-fragment-index="2">
      <span class="card-icon">1998</span>
      <h4>PageRank</h4>
      <p>Eigenvectors of the web graph. A trillion-dollar algorithm.</p>
    </div>
    <div class="summary-card fragment fade-in" data-fragment-index="3">
      <span class="card-icon">2004+</span>
      <h4>Social Networks</h4>
      <p>Billions of nodes, graph algorithms at planetary scale.</p>
    </div>
  </div>

  <aside class="notes">
    Act II applied the pure-math foundations to real networks. Watts&ndash;Strogatz
    (1998) characterized the small-world property via clustering coefficient and
    average path length. PageRank (1998) cast web search as an eigenvector
    problem on a stochastic matrix, with convergence guaranteed by
    Perron&ndash;Frobenius. Social network analysis at scale deployed community
    detection, centrality, and epidemic models on graphs with billions of
    vertices. The code of the universe scales from Euler&rsquo;s 4-node
    multigraph to Facebook&rsquo;s 3-billion-node social graph.
  </aside>
</section>


<!-- ============================================================
     SLIDE 28 — Act III Title (Section Divider)
     ============================================================ -->
<section>
  <div class="section-divider">
    <p class="act-label">Act III</p>
    <h2>Intelligence Is a Graph</h2>
    <p class="act-subtitle">How graph theory became the architecture of AI</p>
  </div>

  <aside class="notes">
    Act III reveals the deepest connection: every neural network is a
    directed acyclic graph (or cyclic, for RNNs), and every transformer
    layer is a message-passing operation on a complete graph. Graph theory
    is not merely useful to AI &mdash; it is the structural language in
    which neural architectures are defined. The code of the universe now
    writes itself in computation graphs and adjacency matrices.
  </aside>
</section>


<!-- ============================================================
     SLIDE 29 — Neural Networks as Graphs
     ============================================================ -->
<section>
  <h2>Neural Networks <em>Are</em> Graphs</h2>
  <figure>
    <img src="images/11-nn-architectures.png" alt="Four neural network architectures shown as graphs">
  </figure>
  <p class="fragment fade-up mt-sm">
    Every neural network is a <strong>directed graph</strong>: neurons are nodes, weights are edges.
  </p>
  <p class="fragment fade-up text-sm hl-muted">
    The architecture revolution (2012&ndash;2017): from chains to grids to complete graphs.
  </p>

  <aside class="notes">
    Every neural network defines a computational graph: a DAG whose vertices
    are operations (matmul, activation, softmax) and whose edges carry tensor
    data. Backpropagation is reverse-mode automatic differentiation on this
    DAG &mdash; it computes gradients by traversing edges in reverse
    topological order, applying the chain rule at each vertex. A feedforward
    net is a path graph. A CNN is a grid-structured graph with weight-sharing
    (translation equivariance). An RNN adds back-edges, making the graph
    cyclic. A transformer is a complete graph at every attention layer.
    The architecture revolution from 2012 to 2017 was a progression through
    graph topologies.
  </aside>
</section>


<!-- ============================================================
     SLIDE 30 — The Transformer as a Complete Graph
     ============================================================ -->
<section>
  <h2>The Transformer: A Complete Graph</h2>
  <figure>
    <img src="images/12-attention-complete.png" alt="Attention pattern as a complete graph between tokens">
  </figure>
  <div class="callout callout-yellow fragment fade-up mt-sm">
    <p class="mb-0"><strong>Every token attends to every other token.</strong>
    The attention matrix is the adjacency matrix of a complete directed graph.</p>
  </div>

  <aside class="notes">
    The transformer (Vaswani et al., 2017) instantiates self-attention as a
    complete weighted digraph $K_n$ over $n$ tokens. The attention matrix
    $A \in \mathbb{R}^{n \times n}$ is the row-stochastic adjacency matrix
    of this graph. Because every pair of tokens is connected, computing $A$
    requires $O(n^2)$ time and memory &mdash; the fundamental bottleneck
    of the architecture. For a 100k-token context, that is $10^{10}$
    operations per layer. This quadratic cost drives the entire research
    program on sparse attention, linear attention, and efficient transformers.
  </aside>
</section>

<!-- ============================================================
     SLIDE 30b — Watch: Attention in Action
     ============================================================ -->
<section>
  <h2>Watch: Attention in Action</h2>
  <iframe data-src="https://www.youtube.com/embed/eMlx5fFNoYc?start=150&end=300"
          data-autoplay
          width="1280" height="720"
          allow="autoplay; fullscreen"
          frameborder="0"
          style="max-width:100%; max-height:23vh;">
  </iframe>
  <aside class="notes">
    This 2.5-minute clip from 3Blue1Brown animates the attention mechanism.
    Watch the $Q$ and $K$ projections: their dot product $QK^T$ produces the
    $n \times n$ score matrix &mdash; the raw adjacency matrix of the
    complete token graph. Softmax normalizes each row into a probability
    distribution. Multiplication by $V$ then computes a weighted average
    along edges. After the clip, we formalize each step and derive the
    scaling factor $\sqrt{d_k}$.
  </aside>
</section>


<!-- ============================================================
     SLIDE 31 — Attention is Graph Theory (with vertical deep-dive)
     ============================================================ -->
<section>
  <!-- Main slide -->
  <section>
    <h2>Attention = Graph Theory</h2>
    <div class="formula-box">
      <span class="formula-label">Scaled Dot-Product Attention</span>
      $$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
    </div>
    <div class="columns-3 mt-md">
      <div class="card fragment fade-in" data-fragment-index="1">
        <h4>$QK^T$</h4>
        <p class="text-sm mb-0">= adjacency matrix</p>
        <p class="text-sm mb-0 hl-muted">Edge weights between all token pairs.</p>
      </div>
      <div class="card card-teal fragment fade-in" data-fragment-index="2">
        <h4 style="color:var(--teal);">softmax</h4>
        <p class="text-sm mb-0">= normalize edges</p>
        <p class="text-sm mb-0 hl-muted">Turn raw scores into a probability distribution.</p>
      </div>
      <div class="card card-orange fragment fade-in" data-fragment-index="3">
        <h4 style="color:var(--orange);">$\times V$</h4>
        <p class="text-sm mb-0">= message passing</p>
        <p class="text-sm mb-0 hl-muted">Aggregate neighbor information along edges.</p>
      </div>
    </div>
    <figure class="fragment fade-in mt-sm" data-fragment-index="4">
      <img src="images/20-attention-heatmap.png"
           alt="6x6 attention heatmap for The cat sat on the mat showing weighted connections between all token pairs"
           style="max-height:13vh; max-width:98%;">
      <figcaption class="text-sm hl-muted">Attention weights = edge weights in the token graph</figcaption>
    </figure>
    <p class="text-sm hl-muted fragment fade-up mt-md" data-fragment-index="5">&darr; Press down for complexity analysis</p>

    <aside class="notes">
      Decompose the attention equation graph-theoretically. $QK^T \in
      \mathbb{R}^{n \times n}$: a weighted adjacency matrix of the complete
      digraph on $n$ tokens. The scaling by $\sqrt{d_k}$ controls gradient
      flow through the softmax &mdash; without it, $\text{Var}(q \cdot k) = d_k$
      for independent unit-variance entries, pushing softmax into saturation
      where gradients vanish. Softmax normalizes each row into a stochastic
      vector (outgoing edge weights sum to 1). Multiplication by $V$ is
      message passing: each token computes a convex combination of its
      neighbors&rsquo; value vectors. This is precisely the MPNN framework
      (Gilmer et al., 2017) on a complete graph.
    </aside>
  </section>

  <!-- Vertical sub-slide: Complexity -->
  <section>
    <h3>Attention Complexity: $O(n^2)$</h3>
    <div class="card">
      <p class="text-sm">Because every token attends to every other token, attention is <strong>quadratic</strong> in sequence length.</p>
    </div>
    <table class="fragment fade-in">
      <thead>
        <tr><th>Sequence Length</th><th>Attention Operations</th><th>Analogy</th></tr>
      </thead>
      <tbody>
        <tr><td>512 tokens</td><td>262,144</td><td>A short email</td></tr>
        <tr><td>4,096 tokens</td><td>16.8 million</td><td>A long article</td></tr>
        <tr><td>100,000 tokens</td><td>10 billion</td><td>A novel</td></tr>
      </tbody>
    </table>
    <figure class="fragment fade-in mt-sm">
      <img src="images/22-sparse-dense-attention.png"
           alt="Three 16x16 matrices comparing full attention, sliding window, and Longformer sparse attention patterns"
           style="max-height:16vh; max-width:98%;">
      <figcaption class="text-sm hl-muted">Full attention (complete graph) vs. sparse patterns (sparse graphs)</figcaption>
    </figure>
    <p class="fragment fade-up text-sm hl-muted mt-md">
      This is why researchers are developing <strong>sparse attention</strong> patterns &mdash; replacing the complete graph with a sparser one.
    </p>

    <aside class="notes">
      Self-attention has time and space complexity $O(n^2 d)$ where $n$ is
      the sequence length and $d$ is the head dimension. For $n = 512$,
      $n^2 \approx 2.6 \times 10^5$; for $n = 100{,}000$, $n^2 = 10^{10}$.
      This quadratic bottleneck drove the development of sparse attention
      variants: Longformer (sliding window + global tokens), BigBird
      (random + window + global), and linear-attention methods like Performer
      that approximate $\text{softmax}(QK^T)$ via random feature maps in
      $O(n)$. Each variant replaces the complete graph with a carefully
      designed sparse graph that trades off expressiveness for efficiency.
    </aside>
  </section>

  <!-- Vertical sub-slide: Multi-Head Attention -->
  <section>
    <h2>Multi-Head Attention: Parallel Graphs</h2>
    <figure>
      <img src="images/21-multihead-attention.png"
           alt="Three attention heads showing different edge patterns on the same tokens"
           style="max-height:27vh; max-width:98%;">
      <figcaption class="text-sm hl-muted">Each head discovers different connections — H heads = H parallel graphs</figcaption>
    </figure>
    <aside class="notes">
      Multi-head attention runs $H$ independent attention operations in parallel,
      each with its own learned projections $W_Q^{(h)}, W_K^{(h)}, W_V^{(h)}$.
      Each head constructs a distinct complete graph with different edge weights,
      capturing different relational patterns: positional adjacency, syntactic
      dependencies (determiners to nouns), semantic roles (verbs to objects).
      The outputs are concatenated and projected: $\text{MultiHead} =
      \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W_O$. This is
      equivalent to $H$ parallel message-passing rounds on the same vertex set
      with different edge-weight functions.
    </aside>
  </section>

  <!-- Vertical: Full Attention Derivation -->
  <section>
    <h3>The Full Derivation (Step by Step)</h3>
    <div class="text-sm" style="line-height:1.5;">
      <p><strong>1.</strong> Input: $X \in \mathbb{R}^{n \times d}$ &nbsp;($n$ tokens, $d$ dimensions)</p>
      <p><strong>2.</strong> Project: $Q = XW_Q,\; K = XW_K,\; V = XW_V$ &nbsp;where $W \in \mathbb{R}^{d \times d_k}$</p>
      <p><strong>3.</strong> Scores: $S = QK^T \in \mathbb{R}^{n \times n}$ &nbsp;&mdash; <em>this IS the adjacency matrix</em></p>
      <p><strong>4.</strong> Scale: $S / \sqrt{d_k}$ &nbsp;&mdash; because $\text{Var}(q \cdot k) = d_k$ for unit-variance entries</p>
      <p><strong>5.</strong> Normalize: $A_{ij} = \frac{\exp(S_{ij}/\sqrt{d_k})}{\sum_k \exp(S_{ik}/\sqrt{d_k})}$ &nbsp;&mdash; softmax &rarr; probability</p>
      <p><strong>6.</strong> Output: $\text{Attn} = AV$ &nbsp;&mdash; weighted average of value vectors</p>
    </div>

    <aside class="notes">
      Step-by-step derivation. (1) Input $X \in \mathbb{R}^{n \times d}$:
      each row is a token embedding. (2) Learned projections $Q = XW_Q$,
      $K = XW_K$, $V = XW_V$ with $W \in \mathbb{R}^{d \times d_k}$. (3)
      Score matrix $S = QK^T \in \mathbb{R}^{n \times n}$ &mdash; the
      weighted adjacency matrix of $K_n$. (4) Scaling: $S / \sqrt{d_k}$.
      Why? For entries with zero mean and unit variance, $\text{Var}(q_i
      \cdot k_j) = d_k$, so dividing by $\sqrt{d_k}$ restores unit variance
      and prevents softmax saturation. (5) Softmax row-normalizes into a
      stochastic matrix: $A_{ij} = \exp(S_{ij}/\sqrt{d_k}) / \sum_k
      \exp(S_{ik}/\sqrt{d_k})$. (6) Output $= AV$: a weighted average of
      value vectors &mdash; message passing on the complete graph. The
      entire computation fuses graph theory with linear algebra.
    </aside>
  </section>
</section>


<!-- ============================================================
     SLIDE — The Mathematics Inside an LLM
     ============================================================ -->
<section>
  <h2>The Mathematics Inside an LLM</h2>
  <figure>
    <img src="images/27-llm-cross-section.png"
         alt="LLM architecture cross-section with mathematical labels at each layer"
         style="max-height:22vh; max-width:98%;">
  </figure>
  <div class="columns-3 mt-sm text-sm">
    <div class="card fragment fade-in" data-fragment-index="1">
      <h4 style="color:var(--teal);">Embedding</h4>
      <p class="mb-0">Linear algebra<br>(high-dim geometry)</p>
    </div>
    <div class="card card-yellow fragment fade-in" data-fragment-index="2">
      <h4 style="color:var(--yellow);">Attention</h4>
      <p class="mb-0">Graph theory +<br>linear algebra</p>
    </div>
    <div class="card card-orange fragment fade-in" data-fragment-index="3">
      <h4 style="color:var(--orange);">Training</h4>
      <p class="mb-0">Calculus + probability<br>(gradient descent, cross-entropy)</p>
    </div>
  </div>

  <aside class="notes">
    Cross-section of an LLM, layer by layer. Embedding: a lookup table
    $E \in \mathbb{R}^{|V| \times d}$ mapping tokens to vectors in
    high-dimensional space &mdash; pure linear algebra. Attention: constructs
    and reads a complete-graph adjacency matrix per head &mdash; graph theory
    meets linear algebra. Feed-forward: two-layer MLP with nonlinearity,
    invoking the universal approximation theorem. Layer normalization:
    standardizes activations, stabilizing gradient flow. Training: gradient
    descent on cross-entropy loss $\mathcal{L} = -\sum_t \log p(x_t | x_{&lt;t})$
    &mdash; calculus and probability. Every layer is a different chapter of
    the code of the universe, working in concert.
  </aside>
</section>


<!-- ============================================================
     SLIDE — Why Does Scaling Work?
     ============================================================ -->
<section>
  <h2>Why Does Scaling Work?</h2>
  <div class="columns">
    <div class="card">
      <h4>Erdos-Renyi (1959)</h4>
      <p class="text-sm mb-0">Giant component at $p > 1/n$</p>
      <p class="text-sm mb-0 hl-muted">A quantitative change &rarr; qualitative leap.</p>
    </div>
    <div class="card card-yellow">
      <h4 style="color:var(--yellow);">Kaplan et al. (2020)</h4>
      <p class="text-sm mb-0">$L(N) \approx (N_c / N)^{\alpha_N}$, &nbsp; $\alpha_N \approx 0.076$</p>
      <p class="text-sm mb-0 hl-muted">Loss follows a power law with model size.</p>
    </div>
  </div>
  <p class="fragment fade-up text-lg hl-teal mt-sm">
    Same pattern: <strong>scale past a critical threshold</strong> and new capabilities emerge.
  </p>

  <aside class="notes">
    Kaplan et al. (2020) established empirical scaling laws: test loss
    $L(N) \approx (N_c / N)^{\alpha_N}$ with $\alpha_N \approx 0.076$
    for model size $N$. This smooth power-law decay coexists with sharp
    capability thresholds &mdash; arithmetic, chain-of-thought, code
    generation &mdash; that appear suddenly at critical scales. Compare
    with the Erd&#337;s&ndash;R&eacute;nyi threshold: for $c &lt; 1$
    the graph is fragmented; for $c &gt; 1$ a giant component emerges
    discontinuously. Whether LLM emergence is a true phase transition
    or an artifact of metric choice (Schaeffer, 2023) is debated, but
    the mathematical analogy is precise: a quantitative increase in a
    control parameter triggers a qualitative structural change.
  </aside>
</section>


<!-- ============================================================
     SLIDE 32 — Knowledge Graphs
     ============================================================ -->
<section>
  <h2>Knowledge Graphs</h2>
  <div class="columns-4060">
    <div>
      <img src="images/13-knowledge-graph.png" alt="Knowledge graph with entity-relation triples" style="max-height:22vh;">
    </div>
    <div>
      <p>Knowledge graphs represent facts as <strong>triples</strong>:</p>
      <div class="formula-box fragment fade-in" data-fragment-index="1" style="text-align:left;">
        <p class="mb-0" style="font-size:1.1em;">
          (<span class="hl-blue">Einstein</span>, <span class="hl-teal">born_in</span>, <span class="hl-orange">Ulm</span>)<br>
          (<span class="hl-blue">Einstein</span>, <span class="hl-teal">developed</span>, <span class="hl-orange">Relativity</span>)<br>
          (<span class="hl-blue">Relativity</span>, <span class="hl-teal">type_of</span>, <span class="hl-orange">Physics Theory</span>)
        </p>
      </div>
      <p class="fragment fade-up text-sm mt-md" data-fragment-index="2">
        <strong>Google Knowledge Graph:</strong> 500+ billion facts, 5+ billion entities.
      </p>
    </div>
  </div>

  <aside class="notes">
    A knowledge graph is a labeled directed multigraph where vertices are
    entities and edges are typed relations, stored as (subject, predicate,
    object) triples. Google&rsquo;s Knowledge Graph contains 500+ billion
    facts over 5+ billion entities. Formally, multi-hop queries reduce to
    path-finding in this graph: &ldquo;Where was the developer of relativity
    born?&rdquo; is a 2-hop traversal (Einstein) $\xrightarrow{\text{developed}}$
    (Relativity) $\xleftarrow{\text{born\_in}}$ (Ulm). Knowledge graph
    embeddings (TransE, RotatE) learn low-dimensional vector representations
    that preserve this relational structure.
  </aside>
</section>


<!-- ============================================================
     SLIDE — RAG & GraphRAG
     ============================================================ -->
<section>
  <h2>RAG &amp; GraphRAG: Giving AI a Memory</h2>
  <div class="columns">
    <div>
      <img src="images/14-rag-pipeline.png" alt="RAG pipeline" style="max-height:16vh;">
      <p class="text-sm hl-muted text-center">Retrieval-Augmented Generation</p>
    </div>
    <div>
      <div class="card fragment fade-in" data-fragment-index="1">
        <h4>RAG</h4>
        <p class="text-sm mb-0">Query a knowledge base &rarr; feed retrieved facts into the LLM &rarr; less hallucination.</p>
      </div>
      <div class="card card-teal fragment fade-in" data-fragment-index="2">
        <h4 style="color:var(--teal);">GraphRAG (2024)</h4>
        <p class="text-sm mb-0">Replace flat retrieval with <strong>graph traversal</strong>: community detection on knowledge graphs enables multi-hop reasoning.</p>
      </div>
    </div>
  </div>

  <aside class="notes">
    RAG (Lewis et al., 2020) grounds generation in retrieved evidence,
    reducing hallucination. Standard RAG uses vector similarity for retrieval
    &mdash; essentially nearest-neighbor search in embedding space. GraphRAG
    (Microsoft, 2024) replaces flat retrieval with graph traversal: it builds
    a knowledge graph from source documents, runs community detection (e.g.,
    Leiden algorithm) to identify topical clusters, and retrieves subgraph
    neighborhoods relevant to the query. This enables multi-hop reasoning
    that flat retrieval cannot support. Here the mathematical thread connects
    community detection algorithms to LLM reliability.
  </aside>
</section>


<!-- ============================================================
     SLIDE 35 — Graph Neural Networks
     ============================================================ -->
<section>
  <h2>Graph Neural Networks (GNNs)</h2>
  <div class="columns-4060">
    <div>
      <img src="images/15-gnn-message.png" alt="GNN message passing illustration" style="max-height:22vh;">
    </div>
    <div>
      <p><strong>GNNs:</strong> Neural networks that learn directly on graph-structured data.</p>
      <div class="card fragment fade-in mt-md" data-fragment-index="1">
        <h4>Message Passing</h4>
        <ol class="text-sm">
          <li class="fragment fade-in" data-fragment-index="2">Each node collects messages from its neighbors.</li>
          <li class="fragment fade-in" data-fragment-index="3">Messages are aggregated (sum, mean, max).</li>
          <li class="fragment fade-in" data-fragment-index="4">Node updates its representation.</li>
          <li class="fragment fade-in" data-fragment-index="5">Repeat for $L$ rounds &mdash; information flows $L$ hops.</li>
        </ol>
      </div>
    </div>
  </div>

  <aside class="notes">
    GNNs generalize transformers to arbitrary graph topologies. The unifying
    abstraction is the Message Passing Neural Network (MPNN) framework
    (Gilmer et al., 2017): each vertex $v$ aggregates messages from its
    neighborhood $\mathcal{N}(v)$, then updates its hidden state. After $L$
    message-passing rounds, each vertex&rsquo;s representation encodes
    information from its $L$-hop neighborhood. GCN, GraphSAGE, and GAT are
    all special cases of MPNN with different aggregation functions. The key
    difference from transformers: GNNs operate on whatever graph structure
    the data provides, rather than constructing a complete graph.
  </aside>
</section>


<!-- ============================================================
     SLIDE 36 — GNN Formula (with vertical deep-dive)
     ============================================================ -->
<section>
  <!-- Main slide -->
  <section>
    <h2>The GNN Formula</h2>
    <div class="formula-box">
      <span class="formula-label">Message Passing Update Rule</span>
      $$h_v^{(l+1)} = \text{UPDATE}\Big(h_v^{(l)},\; \text{AGG}\big(\{h_u^{(l)} : u \in \mathcal{N}(v)\}\big)\Big)$$
    </div>
    <div class="columns mt-md">
      <div class="card card-teal fragment fade-in" data-fragment-index="1">
        <h4 style="color:var(--teal);">AGGREGATE</h4>
        <p class="text-sm mb-0">Collect neighbor features: sum, mean, or attention-weighted.</p>
      </div>
      <div class="card card-orange fragment fade-in" data-fragment-index="2">
        <h4 style="color:var(--orange);">UPDATE</h4>
        <p class="text-sm mb-0">Combine own features with aggregated message. Apply a neural network.</p>
      </div>
    </div>
    <p class="text-sm hl-muted fragment fade-up mt-md" data-fragment-index="3">&darr; Press down for expressiveness theory</p>

    <aside class="notes">
      This is the MPNN update rule in its general form. AGG collects neighbor
      representations &mdash; sum for GCN, mean or LSTM for GraphSAGE,
      attention-weighted for GAT. UPDATE combines the aggregated message with
      the node&rsquo;s own representation and applies a learned transformation
      (typically an MLP with nonlinearity). The formula is permutation-invariant
      over neighbors, which is essential since graphs have no canonical node
      ordering. This single equation subsumes virtually all spatial-domain
      GNN variants; they differ only in the choice of AGG and UPDATE.
    </aside>
  </section>

  <!-- Vertical sub-slide: WL test -->
  <section>
    <h3>GNN Expressiveness &amp; the WL Test</h3>
    <div class="card">
      <p class="text-sm">How powerful are GNNs? They are at most as powerful as the <span class="def-term">Weisfeiler-Leman (WL) graph isomorphism test</span>.</p>
    </div>
    <ul class="fragment fade-in">
      <li>The 1-WL test iteratively refines node colors based on neighbor colors.</li>
      <li>Standard message-passing GNNs are <strong>exactly as expressive</strong> as 1-WL (Xu et al., 2019).</li>
      <li>Some graph pairs (e.g., non-isomorphic regular graphs) cannot be distinguished by 1-WL.</li>
      <li>Higher-order GNNs (k-WL) trade expressiveness for computational cost.</li>
    </ul>
    <p class="fragment fade-up text-sm hl-muted mt-md">
      Understanding these limits guides the design of more powerful architectures.
    </p>

    <aside class="notes">
      Xu et al. (GIN, 2019) proved that message-passing GNNs are at most as
      powerful as the 1-WL (color refinement) isomorphism test, and that GIN
      (Graph Isomorphism Network) achieves this upper bound. Consequence:
      there exist pairs of non-isomorphic graphs &mdash; e.g., certain
      $k$-regular graphs &mdash; that 1-WL (and hence any standard MPNN)
      cannot distinguish. Higher-order $k$-WL tests (and corresponding
      $k$-GNNs) are strictly more powerful but have complexity $O(n^k)$.
      This expressiveness hierarchy is an active research frontier and
      directly guides architecture design for molecular and combinatorial
      domains.
    </aside>
  </section>
</section>


<!-- ============================================================
     SLIDE 37 — Drug Discovery with GNNs
     ============================================================ -->
<section>
  <h2>Drug Discovery with GNNs</h2>
  <div class="columns">
    <div>
      <img src="images/16-molecule-graph.png" alt="Molecule represented as a graph" style="max-height:20vh;">
    </div>
    <div class="col-center">
      <p class="text-sm"><strong>Atoms</strong> = nodes, <strong>Bonds</strong> = edges, <strong>Bond types</strong> = edge labels</p>
      <div class="card card-green fragment fade-up" data-fragment-index="1">
        <p class="text-sm mb-0">GNNs predict toxicity, binding affinity, and solubility directly from the molecular graph &mdash; accelerating drug design from years to months.</p>
      </div>
    </div>
  </div>

  <aside class="notes">
    A molecule is naturally a labeled graph: atoms are vertices (with features
    like atomic number and charge), bonds are edges (with type: single, double,
    aromatic). GNNs learn molecular representations via message passing on
    this graph, predicting properties like toxicity, binding affinity, and
    solubility directly from structure &mdash; no hand-crafted descriptors
    needed. DeepMind&rsquo;s GNoME (2023) used GNNs to discover 2.2 million
    new stable crystal structures. The code of the universe encodes chemistry
    as graph theory.
  </aside>
</section>


<!-- ============================================================
     SLIDE 38 — INTERACTIVE: Design Your Architecture
     ============================================================ -->
<section>
  <h2>Interactive: Design a Neural Network</h2>
  <p class="text-sm hl-muted">Click two nodes to add/remove an edge. Then <strong>Analyze</strong> your architecture.</p>
  <div id="interactive-architect" class="interactive-container tall"></div>

  <aside class="notes">
    This interactive lets you construct a neural architecture as a graph.
    Click two vertices to toggle an edge. Try building: (1) a layered DAG
    (feedforward), (2) a layered DAG with skip edges (ResNet), (3) a
    complete graph (transformer-like). The &ldquo;Analyze&rdquo; button
    reports edge density $|E| / \binom{|V|}{2}$, presence of skip connections,
    and architecture classification. Notice how graph density directly
    determines computational cost and information-flow properties.
  </aside>
</section>


<!-- ============================================================
     SLIDE 39 — Transformers vs GNNs
     ============================================================ -->
<section>
  <h2>Transformers vs. GNNs</h2>
  <table>
    <thead>
      <tr>
        <th>Property</th>
        <th><span class="hl-blue">Transformer</span></th>
        <th><span class="hl-teal">GNN</span></th>
      </tr>
    </thead>
    <tbody>
      <tr class="fragment fade-in" data-fragment-index="1">
        <td>Graph type</td>
        <td>Complete graph</td>
        <td>Sparse / custom graph</td>
      </tr>
      <tr class="fragment fade-in" data-fragment-index="2">
        <td>Complexity</td>
        <td>$O(n^2)$</td>
        <td>$O(|\mathcal{E}|)$ &mdash; edges only</td>
      </tr>
      <tr class="fragment fade-in" data-fragment-index="3">
        <td>Best for</td>
        <td>Language, vision, audio</td>
        <td>Molecules, networks, maps</td>
      </tr>
      <tr class="fragment fade-in" data-fragment-index="4">
        <td>Data assumption</td>
        <td>Sequence / grid</td>
        <td>Arbitrary graph</td>
      </tr>
      <tr class="fragment fade-in" data-fragment-index="5">
        <td>Edge weights</td>
        <td>Learned (attention)</td>
        <td>Given or learned</td>
      </tr>
    </tbody>
  </table>
  <figure class="fragment fade-in mt-sm" data-fragment-index="6">
    <img src="images/23-gnn-vs-transformer.png"
         alt="Side-by-side comparison: GNN message passing on sparse graph vs. Transformer attention on complete graph"
         style="max-height:16vh; max-width:98%;">
    <figcaption class="text-sm hl-muted">Both architectures: message passing along graph edges</figcaption>
  </figure>
  <p class="fragment fade-up text-sm hl-muted mt-md" data-fragment-index="7">Both are message-passing on graphs &mdash; they differ in which graph they use.</p>

  <aside class="notes">
    The key insight: a transformer IS a GNN on the complete graph $K_n$.
    Self-attention computes attention-weighted message passing where every
    vertex is a neighbor of every other &mdash; $O(n^2)$. A GNN on a sparse
    graph with $|E| \ll n^2$ runs in $O(|E|)$ per layer but requires the
    graph structure as input. Joshi et al. (2020) and others have formalized
    this: transformers are a special case of the MPNN framework where the
    underlying graph is complete and edge weights are learned via the QKV
    mechanism. Transformers dominate sequence and grid data; GNNs dominate
    molecular, social, and relational data. Both are message passing &mdash;
    they differ only in which graph they operate on.
  </aside>
</section>


<!-- ============================================================
     SLIDE 40 — The Unifying Insight
     ============================================================ -->
<section class="flex-center center-layout">
  <h2>The Unifying Insight</h2>
  <p class="big-quote" style="border-left-color:var(--blue);">
    Every AI architecture is a <span class="hl-yellow">choice of graph topology</span>.
  </p>
  <div class="columns-3 mt-lg">
    <div class="card fragment fade-in" data-fragment-index="1">
      <h4>Feedforward</h4>
      <p class="text-sm mb-0 hl-muted">= chain graph</p>
    </div>
    <div class="card card-teal fragment fade-in" data-fragment-index="2">
      <h4>CNN</h4>
      <p class="text-sm mb-0 hl-muted">= grid / lattice graph</p>
    </div>
    <div class="card card-yellow fragment fade-in" data-fragment-index="3">
      <h4>Transformer</h4>
      <p class="text-sm mb-0 hl-muted">= complete graph</p>
    </div>
  </div>
  <div class="columns-3 mt-sm">
    <div class="card card-orange fragment fade-in" data-fragment-index="4">
      <h4>RNN</h4>
      <p class="text-sm mb-0 hl-muted">= chain with cycles</p>
    </div>
    <div class="card card-green fragment fade-in" data-fragment-index="5">
      <h4>GNN</h4>
      <p class="text-sm mb-0 hl-muted">= custom / data graph</p>
    </div>
    <div class="card card-purple fragment fade-in" data-fragment-index="6">
      <h4>Mixture of Experts</h4>
      <p class="text-sm mb-0 hl-muted">= sparse bipartite graph</p>
    </div>
  </div>

  <aside class="notes">
    Every architecture is a choice of graph topology for the computational
    graph. Feedforward: directed path graph. CNN: grid/lattice with local
    receptive fields and weight sharing. RNN: directed path with back-edges
    (cycles). Transformer: complete digraph per layer. GNN: data-defined
    graph. Mixture of Experts (e.g., Mixtral): sparse bipartite graph where
    a routing function selects $k$ of $N$ experts per token. This is the
    unifying insight of the lecture &mdash; the code of the universe writes
    AI architectures as graph topologies.
  </aside>
</section>


<!-- ============================================================
     SLIDE 41 — The 300-Year Chain
     ============================================================ -->
<section>
  <h2>The 300-Year Chain</h2>
  <figure>
    <img src="images/17-timeline-chain.png" alt="Timeline from Euler 1736 to GNNs 2020s">
  </figure>
  <div class="columns-3 mt-sm text-sm text-center">
    <p class="fragment fade-in" data-fragment-index="1">
      <span class="hl-orange">1736</span> Euler &mdash; <em>abstraction</em><br>
      <span class="hl-muted">1847</span> Kirchhoff &mdash; <em>Laplacian</em><br>
      <span class="hl-muted">1857</span> Cayley &mdash; <em>counting</em><br>
      <span class="hl-muted">1959</span> Erdos &mdash; <em>randomness</em>
    </p>
    <p class="fragment fade-in" data-fragment-index="2">
      <span class="hl-blue">1967</span> Milgram &mdash; <em>experiment</em><br>
      <span class="hl-muted">1998</span> Watts-Strogatz &mdash; <em>models</em><br>
      <span class="hl-muted">1998</span> Page &amp; Brin &mdash; <em>eigenvectors</em>
    </p>
    <p class="fragment fade-in" data-fragment-index="3">
      <span class="hl-green">2017</span> Vaswani &mdash; <em>attention</em><br>
      <span class="hl-muted">2020</span> Kaplan &mdash; <em>scaling laws</em><br>
      <span class="hl-muted">2024</span> GNNs &amp; GraphRAG
    </p>
  </div>

  <aside class="notes">
    The full arc: Euler (1736) &mdash; incidence abstraction and degree-parity
    criterion. Kirchhoff (1847) &mdash; the graph Laplacian and spectral
    counting. Cayley (1857) &mdash; the Pr&uuml;fer bijection and $n^{n-2}$.
    Erd&#337;s &amp; R&eacute;nyi (1959) &mdash; random graphs and phase
    transitions. Milgram (1967) &mdash; empirical small-world property.
    Watts &amp; Strogatz (1998) &mdash; small-world models. Page &amp; Brin
    (1998) &mdash; eigenvector centrality on the web graph. Vaswani et al.
    (2017) &mdash; self-attention as complete-graph message passing. Each
    result built directly on its predecessors. Three centuries, one
    continuous mathematical thread.
  </aside>
</section>


<!-- ============================================================
     SLIDE 42 — The Thread That Connects (Big Quote)
     ============================================================ -->
<section>
  <div class="columns-4060">
    <div class="col-center" style="text-align:center;">
      <img src="images/portraits/euler-handmann-1753.jpg"
           alt="Leonhard Euler"
           style="max-height:17vh; border-radius:12px; box-shadow: 0 4px 20px rgba(0,0,0,0.5);">
    </div>
    <div class="col-center">
      <div class="big-quote" style="font-size:1.3em;">
        "Mathematics is the language in which God has written the universe."
        <span class="attribution">&mdash; Galileo Galilei, <em>Il Saggiatore</em>, 1623</span>
      </div>
      <p class="fragment fade-up mt-md text-lg">
        Euler decoded the language of connections.<br>
        Three hundred years later, we taught machines to read it.
      </p>
    </div>
  </div>

  <aside class="notes">
    Galileo&rsquo;s claim from 1623 is vindicated across four centuries.
    Euler decoded the language of discrete structure in 1736. Cayley,
    Kirchhoff, and Erd&#337;s extended the vocabulary with enumeration,
    spectral methods, and probabilistic models. In the 21st century, we
    taught machines to read this language &mdash; to construct adjacency
    matrices, compute eigenvectors, and learn representations by message
    passing on graphs. The mathematics came first; the AI followed. The
    code of the universe was always there, waiting to be compiled.
  </aside>
</section>


<!-- ============================================================
     SLIDE — The Five Pillars
     ============================================================ -->
<section>
  <h2>The Five Mathematical Pillars of AI</h2>
  <p class="text-sm hl-muted">Today you traced one thread. Here are the others:</p>
  <div class="columns-3 mt-sm text-sm">
    <div class="card card-yellow">
      <h4 style="color:var(--yellow);">Graph Theory</h4>
      <p class="mb-0 hl-muted">Today's thread &check;</p>
    </div>
    <div class="card fragment fade-in" data-fragment-index="1">
      <h4>Linear Algebra</h4>
      <p class="mb-0 hl-muted">Why does king &minus; man + woman = queen?</p>
    </div>
    <div class="card card-teal fragment fade-in" data-fragment-index="2">
      <h4 style="color:var(--teal);">Probability</h4>
      <p class="mb-0 hl-muted">Why does training on text produce reasoning?</p>
    </div>
  </div>
  <div class="columns-3 mt-sm text-sm">
    <div class="card card-orange fragment fade-in" data-fragment-index="3">
      <h4 style="color:var(--orange);">Calculus</h4>
      <p class="mb-0 hl-muted">How do 175B parameters converge to useful behavior?</p>
    </div>
    <div class="card card-green fragment fade-in" data-fragment-index="4">
      <h4 style="color:var(--green);">Information Theory</h4>
      <p class="mb-0 hl-muted">What is the entropy of English?</p>
    </div>
    <div class="card card-purple fragment fade-in" data-fragment-index="5">
      <h4 style="color:var(--purple);">Topology</h4>
      <p class="mb-0 hl-muted">Why do neural nets learn on manifolds?</p>
    </div>
  </div>

  <aside class="notes">
    Today covered one of six mathematical pillars. Linear algebra: word
    embeddings live in $\mathbb{R}^d$ and semantic analogies emerge as vector
    arithmetic &mdash; why? Probability: autoregressive training on
    $P(x_t | x_{&lt;t})$ produces emergent reasoning &mdash; how?
    Calculus: stochastic gradient descent over $10^{11}$ parameters converges
    to useful minima &mdash; what loss landscape geometry enables this?
    Information theory: Shannon entropy of English is roughly 1.0&ndash;1.5
    bits per character &mdash; how does this constrain and enable compression?
    Topology: learned representations concentrate on low-dimensional manifolds
    &mdash; what is the intrinsic dimension? Each is a research frontier and
    each deserves its own lecture.
  </aside>
</section>


<!-- ============================================================
     SLIDE 43 — What Comes Next (Careers)
     ============================================================ -->
<section>
  <h2>What Comes Next</h2>
  <p>Graph theory is one of the most <strong>in-demand</strong> skills in tech. Where can it take you?</p>
  <ul class="career-list mt-lg">
    <li class="fragment fade-in" data-fragment-index="1">AI / Machine Learning Engineer &mdash; <span class="hl-muted text-sm">linear algebra, optimization, graph theory</span></li>
    <li class="fragment fade-in" data-fragment-index="2">Data Scientist &mdash; <span class="hl-muted text-sm">statistics, probability, network analysis</span></li>
    <li class="fragment fade-in" data-fragment-index="3">Quantitative Researcher &mdash; <span class="hl-muted text-sm">stochastic calculus, number theory</span></li>
    <li class="fragment fade-in" data-fragment-index="4">Computational Biologist &mdash; <span class="hl-muted text-sm">graph algorithms, dynamical systems</span></li>
    <li class="fragment fade-in" data-fragment-index="5">Research Mathematician &mdash; <span class="hl-muted text-sm">topology, algebra, analysis (AI is hiring you)</span></li>
    <li class="fragment fade-in" data-fragment-index="6">Cybersecurity Analyst &mdash; <span class="hl-muted text-sm">graph theory, cryptography</span></li>
  </ul>
  <hr class="accent-rule mt-lg">
  <p class="text-sm hl-muted text-center mt-md">
    Thank you. Questions?
  </p>

  <aside class="notes">
    Graph theory is a foundation for multiple career paths. ML engineers deploy
    GNNs and transformer architectures. Data scientists run spectral clustering
    and network analysis at scale. Quantitative researchers model financial
    contagion via network theory and stochastic calculus. Computational
    biologists apply GNNs to protein interaction networks and molecular
    property prediction. Cybersecurity analysts model attack surfaces as
    directed graphs. Research mathematicians work on expressiveness theory,
    spectral graph theory, and topological data analysis &mdash; all in
    high demand at AI labs. Wherever there are connections, graph theory
    is the language. Thank you. Questions welcome.
  </aside>
</section>


<!-- ============================================================
     SLIDE 44 — Image Credits
     ============================================================ -->
<section>
  <h3>Image Credits</h3>
  <ul class="text-sm" style="line-height:1.8;">
    <li>Leonhard Euler portrait: Jakob Emanuel Handmann, 1753 — Public Domain</li>
    <li>Arthur Cayley engraving: Wellcome Collection — CC BY 4.0</li>
    <li>Paul Erdős photograph: Wikimedia Commons</li>
    <li>Larry Page photograph: Wikimedia Commons — CC BY 2.0</li>
    <li>Sergey Brin photograph: Wikimedia Commons — CC BY-SA 2.0</li>
    <li>Königsberg map: Merian-Erben, 1652 — Public Domain</li>
    <li>All generated diagrams: Original work for this lecture</li>
    <li>Mathematical constellation diagram: Original work for this lecture</li>
    <li>LLM architecture cross-section: Original work for this lecture</li>
    <li>Video: "How the Königsberg bridge problem changed mathematics" — TED-Ed (YouTube)</li>
    <li>Video: "Attention in transformers, step-by-step" — 3Blue1Brown (YouTube)</li>
    <li>Video: "Large Language Models explained briefly" — 3Blue1Brown (YouTube)</li>
  </ul>
</section>

</div><!-- .slides -->
</div><!-- .reveal -->

<!-- ============================================================
     SCRIPTS
     ============================================================ -->
<script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reveal.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/plugin/notes/notes.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/plugin/math/math.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script src="js/interactive.js"></script>

<script>
  Reveal.initialize({
    hash: true,
    slideNumber: 'c/t',
    showSlideNumber: 'speaker',
    transition: 'slide',
    transitionSpeed: 'default',
    backgroundTransition: 'fade',
    center: true,
    width: 640,
    height: 360,
    margin: 0.01,
    minScale: 0.2,
    maxScale: 4.0,
    plugins: [RevealNotes, RevealMath.KaTeX],
    katex: {
      local: false,
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false}
      ]
    }
  });
</script>

</body>
</html>
